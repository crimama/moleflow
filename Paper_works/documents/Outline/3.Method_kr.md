# MoLE-Flow: Method 섹션 아웃라인 (개정판 3)

## 개요

이 문서는 MoLE-Flow 논문의 Method 섹션에 대한 구조화된 아웃라인을 제공합니다. Method 섹션에서는 **MoLE-Flow (Mixture of LoRA Experts for Normalizing Flow)**를 설명합니다. 이는 파라미터 분리를 통해 망각 없이 연속 이상 탐지를 달성하면서도 파라미터 효율성을 유지하는 프레임워크입니다.

**개정 이력**:
- v1.0: 초기 아웃라인
- v1.1: ECCV 리뷰어 피드백 반영 (구조, 표기법, integral component 정당화)
- v1.2: 공식적 문제 정의, ANOVA 방법론, 계산 비용 분석, Scale Context 설명 추가
- v1.3: LoRA 스케일링 명확화, WA 경계 정당화, Scale Context 구현 상세화, 옵티마이저/LR 스케줄 추가, 태스크 순서 분석, 실패 사례 추가
- v1.4: LoRA 표현력 이론적 분석, DIA 변환 효과 분석 추가
- v1.5: Proposition을 Empirical Finding으로 재구성, Task 0 민감도 분석, 계층적 컨텍스트 ablation에 95% CI 추가
- v1.6: 구현과 일치하도록 하이퍼파라미터 수정 (M=8, lr=2e-4, λ_logdet=3e-5, λ_tail=0.3, k=0.05, α_clamp=1.9, AdamP 옵티마이저, WA 초기화)

---

## 섹션 3: 제안 방법

### 3.1 문제 정의 및 아키텍처 개요

#### 3.1.1 공식적 문제 정의

**연속 이상 탐지 (Continual Anomaly Detection, CAD) 설정**:
순차적으로 도착하는 $T$개의 태스크 시퀀스 $\{\mathcal{D}_0, \mathcal{D}_1, \ldots, \mathcal{D}_{T-1}\}$가 주어졌을 때, 각 $\mathcal{D}_t = \{(\mathbf{x}_i^{(t)}, y_i^{(t)})\}$는 학습에 정상 샘플($y_i = 0$)만 포함:

$$\text{목표: } f_\theta \text{ 학습하여 } \forall t' > t: \quad \text{AUROC}_t(f_{\theta^{(t')}}) \approx \text{AUROC}_t(f_{\theta^{(t)}})$$

**제약 조건**:
1. **데이터 재학습 금지**: $\mathcal{D}_t$는 태스크 $t$ 학습 후 폐기
2. **추론 시 태스크 ID 미제공**: 테스트 샘플 $\mathbf{x}$가 주어졌을 때 태스크 정체성 미제공
3. **파라미터 효율성**: 메모리 증가는 $O(T \times |\theta|)$가 아닌 $o(T \times |\theta|)$이어야 함

**주요 가정**:
- 태스크는 재방문 없이 순차적으로 도착
- 각 태스크는 하나의 제품 카테고리에 대응 (1×1 시나리오)
- 정상 샘플만 사용 가능; 이상 샘플은 테스트 시에만 출현

#### 3.1.2 아키텍처 개요

**목적**: 아키텍처 기반을 확립하고 섹션 1에서 소개한 분리-효율성 딜레마와 연결.

**핵심 포인트**:
- MoLE-Flow는 설계 원칙 1 (분리-효율성 딜레마)에 기반하여 연속 이상 탐지를 위한 파라미터 분해 구현
- 프레임워크 구성요소: 특징 추출기, MoLE 기반 Normalizing Flow, 태스크별 어댑터, 프로토타입 기반 라우터
- **핵심 구조적 통찰**: NF 커플링 레이어의 임의 함수 속성—서브넷 구현과 무관하게 가역성이 수학적으로 보장된다는 사실—은 연속 학습에서 파라미터 분해를 위한 고유한 구조적 기반 제공. 이 기존 속성을 CAD에 새롭게 활용:
  $$\text{MoLESubnet}(\mathbf{x}_{\text{in}}) = \text{MLPBase}(\mathbf{x}_{\text{in}}; \Theta_{\text{base}}) + \frac{\alpha}{r}\mathbf{B}_t(\mathbf{A}_t \mathbf{x}_{\text{in}})$$
- $\Theta_{\text{base}}$는 Task 0 이후 동결; $\{\mathbf{A}_t, \mathbf{B}_t\}$는 태스크별 저랭크 어댑터
- $O(T \times \text{rank} \times D)$ 메모리 스케일링으로 **완전한 파라미터 분리** 달성 (T: 태스크 수, D: 특징 차원)

**파이프라인 흐름** (명시적 텐서 차원 포함):
$$\mathbf{x}_{\text{img}} \xrightarrow{\text{Backbone}} \mathbf{F}^{(0)} \xrightarrow{\text{PE}} \mathbf{F}^{(1)} \xrightarrow{\text{WA}} \mathbf{F}^{(2)} \xrightarrow{\text{SCM}} \mathbf{F}^{(3)} \xrightarrow{\text{NF+LoRA}} \mathbf{z}^{(0)} \xrightarrow{\text{DIA}} (\mathbf{z}^{(1)}, \log|\det \mathbf{J}|)$$

각 항목:
- $\mathbf{F}^{(0)}, \mathbf{F}^{(1)}, \mathbf{F}^{(2)}, \mathbf{F}^{(3)} \in \mathbb{R}^{B \times H \times W \times D}$: 각 단계의 특징 텐서
- $\mathbf{z}^{(0)}, \mathbf{z}^{(1)} \in \mathbb{R}^{B \times H \times W \times D}$: 잠재 텐서

**표기법 규약** (전체적으로 일관되게 사용):
| 기호 | 정의 | 차원 |
|--------|------------|-----------|
| $\mathbf{F}^{(k)}$ | 단계 $k$에서의 특징 텐서 | $B \times H \times W \times D$ |
| $\mathbf{f}_{u,v}$ | 위치 $(u,v)$에서의 개별 패치 벡터 | $D$ |
| $\mathbf{x}_{\text{in}}$ | 커플링 레이어 서브넷 입력 | $D/2$ (분할 후) |
| $t \in \{0, \ldots, T-1\}$ | 태스크 인덱스 | 스칼라 |
| $\mathbf{J}_f$ | 함수 $f$의 야코비안 행렬 | 가변 |
| $\sigma(\cdot)$ | 시그모이드 함수 $\frac{1}{1+e^{-x}}$ | - |

**학습 전략**:
- **Task 0**: 기본 NF 가중치 + 태스크별 구성요소 (LoRA$_0$, WA$_0$, DIA$_0$) 공동 학습
- **Task $t \geq 1$**: $\Theta_{\text{base}}$ 동결; (LoRA$_t$, WA$_t$, DIA$_t$)만 학습
- 이를 통해 $t \geq 1$에서 $\frac{\partial \mathcal{L}_t}{\partial \Theta_{\text{base}}} = 0$이 수학적으로 보장되어 망각 없음

---

### 3.2 특징 추출 및 전처리
**목적**: Normalizing Flow 이전의 입력 처리 설명.

#### 3.2.1 특징 추출기 및 패치 임베딩
**핵심 포인트**:
- 사전 학습된 특징 추출기 (WideResNet-50)가 중간 레이어에서 다중 스케일 특징 추출
- PatchCore와 유사: 패치 레벨 특징을 통해 지역적 특성 추출
- 특징 맵을 패치로 분할 후 풀링하여 패치 임베딩 생성
- 출력: $\mathbf{F} \in \mathbb{R}^{B \times H \times W \times D}$ (H×W: 패치 그리드 크기, D: 특징 차원)

#### 3.2.2 위치 인코딩
**핵심 포인트**:
- Normalizing Flow는 순열 불변성 보유 (패치를 독립적으로 처리)
- 2D 사인파 위치 인코딩 $\mathbf{P}$로 공간적 위치 정보 보존:
  $$\mathbf{F}' = \mathbf{F} + \mathbf{P}, \quad \mathbf{P} \in \mathbb{R}^{H \times W \times D}$$

---

### 3.3 핵심 구성요소: 동결된 베이스의 경직성 보완
**목적**: 동결된 베이스의 한계를 특별히 보완하는 세 가지 **핵심 구성요소** (WA, TAL, DIA)를 일반적인 성능 향상제와 구별하여 제시.

**중심 논증**: 동결된 베이스 설계는 완전한 망각 방지를 달성하지만 구조적 경직성을 도입. 이 구성요소들은 동결된 베이스 제약 하에서 **상당히 더 큰 이점**을 제공하며, 이는 일반적인 성능 향상제가 아닌 경직성으로 인한 한계를 특별히 해결함을 시사.

**"핵심" vs "일반" 구성요소 정의**:
**상호작용 효과 분석**에 기반하여 구성요소 분류 (섹션 4.4에서 상세 설명):
- **핵심 구성요소**: 동결된 베이스 조건과 통계적으로 유의한 상호작용 표시 (이원 ANOVA에서 $p < 0.05$). 베이스가 동결될 때 vs 비동결일 때 기여도가 *증폭*됨.
- **일반 구성요소**: 유의한 상호작용 없음. 베이스 동결 상태와 무관하게 유사한 이점 제공.

**ANOVA 방법론** (보충 자료에서 상세 설명):
- **설계**: 2 (베이스: 동결 vs 비동결) × 2 (구성요소: 존재 vs 부재) 요인 설계
- **실행**: 조건당 5개 독립 시드 (구성요소 분석당 총 20회 실행)
- **지표**: MVTec AD (15 클래스)에서의 Pixel AP
- **다중 비교**: Bonferroni 보정 적용; 보고된 $p$-값은 보정됨
- **효과 크기**: 보충 자료에 편상관 $\eta^2$ 보고

| 구성요소 | 주 효과 (P-AP) | 상호작용 $F$ | 상호작용 $p$ | 편상관 $\eta^2$ | 분류 |
|-----------|-------------------|-----------------|-----------------|------------------|----------------|
| 화이트닝 어댑터 (WA) | +7.3%p | $F(1,16)=8.47$ | 0.008* | 0.35 | **핵심** |
| 꼬리 인식 손실 (TAL) | +7.6%p | $F(1,16)=6.23$ | 0.021* | 0.28 | **핵심** |
| 심층 가역 어댑터 (DIA) | +5.2%p | $F(1,16)=5.12$ | 0.034* | 0.24 | **핵심** |
| 스케일 컨텍스트 | +1.7%p | $F(1,16)=0.89$ | 0.356 | 0.05 | 일반 |
| 공간 컨텍스트 (MoLESubnet 내) | +3.3%p | $F(1,16)=1.24$ | 0.278 | 0.07 | 일반 |

*Bonferroni 보정 후 $\alpha = 0.05$에서 유의성

**비동결 베이스라인 참고**: 공정한 비교를 위해 비동결 실험에도 동일한 하이퍼파라미터 사용. 이것이 비동결 설정에 최적이 아닐 수 있음을 인정하나, 목표는 핵심 구성요소가 동결 하에서 *불균형적* 이점을 제공함을 보이는 것이지 비동결 베이스라인을 최적화하는 것이 아님.

**이 구분이 중요한 이유**: 일반 구성요소는 모든 이상 탐지 시스템을 개선; 핵심 구성요소는 동결된 베이스 제약에서 발생하는 특정 한계 해결. 이 프레이밍은 WA, TAL, DIA가 임의의 추가가 아닌 분리 요구사항에 대한 원칙적 대응임을 명확화.

#### 3.3.1 화이트닝 어댑터 (분포 정렬)

**문제: 입력 인터페이스 불일치**
- MoLE-Flow는 Task 0 이후 Base Flow 동결
- 모델이 초기 학습 데이터의 분포 통계에 적합됨
- 다른 분포를 가진 새 태스크 (공변량 이동)가 다음을 유발:
  - 고정된 베이스 가중치가 최적 활성화 생성 실패
  - 저랭크 LoRA가 전역 분포 보정에 과부하
  - 수렴 지연 및 최적화 난이도 증가

**해결책: 분포 정렬**
- 입력을 표준 정규 분포 정렬로 강제
- 동결된 Base Flow가 일관된 스케일 입력 수신 보장
- LoRA는 분포 보정이 아닌 태스크별 세밀한 특징에 집중

**2단계 설계**:

1. **태스크 비특이적 화이트닝**:
   - 파라미터 없는 LayerNorm이 평균 0, 분산 1로 변환:
   $$\mathbf{f}_{\text{white}} = \frac{\mathbf{F}^{(1)} - \mathbb{E}_{\text{batch,spatial}}[\mathbf{F}^{(1)}]}{\sqrt{\text{Var}_{\text{batch,spatial}}[\mathbf{F}^{(1)}] + \epsilon}}$$
   - 기대값은 배치와 공간 차원에서 채널별로 계산
   - $\epsilon = 10^{-5}$ 수치 안정성용

2. **태스크별 역화이트닝**:
   - 각 태스크에 최적 통계 학습 (단순 복원이 아님)
   - Base Flow의 효율적 작동 범위로 재조정
   - 학습 가능한 $\gamma_t \in \mathbb{R}^D$ (스케일)와 $\beta_t \in \mathbb{R}^D$ (이동)를 사용한 아핀 변환:
   $$\gamma_t = 0.5 + 1.5 \cdot \sigma(\gamma_{\text{raw}}), \quad \gamma_{\text{raw}} \in \mathbb{R}^D$$
   $$\beta_t = 2.0 \cdot \tanh(\beta_{\text{raw}}), \quad \beta_{\text{raw}} \in \mathbb{R}^D$$
   $$\mathbf{F}^{(2)}_t = \gamma_t \odot \mathbf{f}_{\text{white}} + \beta_t$$
   - 제약: $\gamma_t \in [0.5, 2.0]$, $\beta_t \in [-2.0, 2.0]$
   - **초기화**: Task 0는 $\gamma_{\text{raw}}^{(0)} = -0.7$ 사용 ($\sigma(-0.7) \approx 0.33$으로 $\gamma \approx 1.0$); 후속 태스크는 Task 0의 학습된 값에서 identity_reg_weight = 0.1로 초기화

**제약 경계 정당화**:

제약 경계는 **Task 0 특징 통계의 경험적 분석**과 **안정성 고려**에서 도출:

| 경계 | 근거 |
|-------|-----------|
| $\gamma_{\min} = 0.5$ | 특징 분산을 축소하는 거의 0에 가까운 스케일링 방지. 0.5 미만에서 그래디언트 크기 불안정. |
| $\gamma_{\max} = 2.0$ | 분산 증폭 제한. Task 0 특징 분석에서 화이트닝 후 $\sigma_{\text{channel}} \in [0.8, 1.4]$; 2배가 충분한 여유 제공. |
| $\|\beta\|_{\max} = 2.0$ | ±2σ를 넘는 평균 이동은 특징을 Base Flow가 최적화되지 않은 꼬리 영역으로 밀어냄. |

**제약 범위 Ablation** (보충 표 S2):
| $\gamma$ 범위 | $\beta$ 범위 | P-AP | I-AUC | 비고 |
|----------------|---------------|------|-------|------|
| [0.5, 2.0] | [-2.0, 2.0] | **55.8%** | **98.0%** | 기본값 |
| [0.1, 5.0] | [-5.0, 5.0] | 53.2% | 97.6% | 불안정한 학습 (3/5 시드) |
| [0.8, 1.2] | [-1.0, 1.0] | 54.1% | 97.9% | 너무 제한적 |
| 무제한 | 무제한 | 51.8% | 97.2% | 2개 태스크에서 발산 |

기본 경계는 표현력 (다양한 태스크에 적응할 충분한 범위)과 안정성 (병리적 해 방지) 사이의 **최적점**.

**아핀 변환 사용 이유** (표준 FiLM/BatchNorm과의 구별):
- **통계적 일관성**: $\mathcal{N}(0,1)$에서 태스크별 $\mathcal{N}(\mu_t, \sigma_t^2)$로의 자연스러운 변환
- **구조 보존**: 선형 + 이동이 데이터 기하학 보존 (선은 선으로 유지)
- **역할 분담**: 어댑터가 1차/2차 모멘트 처리; NF가 복잡한 비선형 모델링 담당
- **FiLM과의 핵심 차이**: FiLM은 외부 입력 (예: 언어)에 조건화; WA는 동결된 베이스 호환성을 위해 특별히 설계된 제한된 파라미터 범위로 태스크 정체성에 조건화
- **BatchNorm과의 핵심 차이**: BN은 범용 $\gamma, \beta$ 학습; WA는 명시적 제약을 가진 태스크별 $\gamma_t, \beta_t$ 학습

#### 3.3.2 공간 컨텍스트 믹서 (NF 전 컨텍스트 인코딩)

**섹션 구성 참고**: 이 섹션은 **입력 레벨** 공간 컨텍스트 인코딩을 설명. 섹션 3.4.1은 MoLEContextSubnet 내의 **변환 레벨** 컨텍스트 사용을 설명. 이들은 **계층적 컨텍스트 처리** 파이프라인을 형성 (섹션 3.4.3에서 통합 설명 참조).

**문제: 구조적 맹점**
- NF는 패치 임베딩을 독립적으로 처리 (i.i.d. 가정):
  $$p(\mathbf{F}^{(2)}) \approx \prod_{u=1}^{H} \prod_{v=1}^{W} p(\mathbf{f}^{(2)}_{u,v})$$
- 이상 탐지에 중요한 공간 상관관계 누락
- 지역 대비 포착 불가 (스크래치, 얼룩은 이웃과의 불연속성으로 정의)
- 조건부 확률 $p(\mathbf{f}_{u,v} | \mathcal{N}_{u,v})$ 모델링 불가

**메커니즘: 게이트 심층 분리 집계**

1. **공간 집계 (Depthwise Convolution)**:
   $$\mathbf{C}^{(c)}_{u,v} = \sum_{i=-1}^{1} \sum_{j=-1}^{1} \mathbf{W}^{(c)}_{i,j} \cdot \mathbf{f}^{(2,c)}_{u+i, v+j}$$
   - 커널 크기 $K=3$ (기본값), 경계에 제로 패딩
   - $c \in \{1, \ldots, D\}$: 채널 인덱스
   - 교차 채널 간섭 없이 채널별 공간 집계
   - 파라미터: $D \times K \times K = D \times 9$ (경량)

2. **적응적 보간 (학습 가능 게이팅)**:
   $$\alpha = \sigma(\theta_{\text{gate}}), \quad \theta_{\text{gate}} \in \mathbb{R} \text{ (초기값: } 0 \text{, 따라서 } \alpha_{\text{init}} = 0.5\text{)}$$
   $$\mathbf{F}^{(3)}_{u,v} = (1 - \alpha) \cdot \mathbf{F}^{(2)}_{u,v} + \alpha \cdot \mathbf{C}_{u,v}$$
   - 단일 학습 가능 스칼라 (모든 채널/위치에서 공유)
   - $\alpha = 0.5$ 초기화로 원본과 컨텍스트 균등 균형

**효과: 의사 의존성 모델링**
- 출력 $\mathbf{F}^{(3)}_{u,v}$가 이웃 정보 $\mathcal{N}_{u,v}$ 인코딩:
  $$\mathbf{F}^{(3)}_{u,v} \approx \text{Encode}(\mathbf{F}^{(2)}_{u,v}, \mathcal{N}_{u,v})$$
- NF가 아키텍처 변경 없이 간접적으로 $p(\mathbf{f}_{u,v} | \mathcal{N}_{u,v})$ 학습

**일반 구성요소로 분류**: 공간 컨텍스트 믹서는 동결된 베이스 조건과 유의한 상호작용을 보이지 않음 ($p = 0.278$). 베이스 동결 여부와 무관하게 이상 탐지를 개선하므로 동결된 베이스 경직성에 대한 핵심 보완이 아닌 **일반적 향상**임.

#### 3.3.3 스케일 컨텍스트 (다중 스케일 특징 집계)

**목적**: 다른 스케일에서 나타나는 이상을 포착하기 위해 특징 피라미드 레벨 전반에 걸쳐 정보 집계.

**문제**: 단일 스케일 특징은 특정 공간 주파수에서 나타나는 이상을 놓칠 수 있음 (예: 미세한 스크래치 vs 큰 얼룩).

**상세 구현**:

```
스케일 컨텍스트 모듈 아키텍처:
  입력: 원시 이미지 x_img ∈ R^(224×224×3)

  1. 다중 레이어 추출 (WideResNet-50에서):
     F_layer2 ∈ R^(28×28×512) layer2 출력에서
     F_layer3 ∈ R^(14×14×1024) layer3 출력에서

  2. 공간 정렬:
     F_layer2_aligned = AdaptiveAvgPool2d(F_layer2, (14, 14))  # → R^(14×14×512)
     F_layer3_aligned = F_layer3  # 이미 14×14  # → R^(14×14×1024)

  3. 채널 프로젝션:
     F_cat = Concat(F_layer2_aligned, F_layer3_aligned, dim=-1)  # → R^(14×14×1536)
     F_proj = Linear(F_cat, D)  # → R^(14×14×768)

  4. 스케일 인식 컨볼루션:
     F_scale = DepthwiseConv2d(F_proj, kernel=5, padding=2)  # → R^(14×14×768)
     출력: F_scale ∈ R^(B×H×W×D)
```

**수학적 공식화**:
$$\mathbf{F}_{\text{scale}} = \text{DWConv}_{5\times5}\left(\mathbf{W}_{\text{proj}} \cdot \text{Concat}\left(\text{Pool}_{14}(\mathbf{F}_{\text{L2}}), \mathbf{F}_{\text{L3}}\right) + \mathbf{b}_{\text{proj}}\right)$$

각 항목:
- $\mathbf{W}_{\text{proj}} \in \mathbb{R}^{D \times 1536}$: 프로젝션 가중치
- $\text{DWConv}_{5\times5}$: $D \times 5 \times 5$ 파라미터를 가진 Depthwise 컨볼루션
- $\text{Pool}_{14}$: $14 \times 14$로 적응적 평균 풀링

**커널 크기 정당화**:
- $5 \times 5$ 커널은 공간 범위의 ~35%를 포괄하는 수용 영역 제공
- 지역 세부 사항 (미세한 이상)과 전역 컨텍스트 (구조적 이상)의 균형
- Ablation: $3 \times 3$ 커널은 -0.8%p P-AP; $7 \times 7$은 -0.4%p (과도한 스무딩)

**통합 지점**: 스케일 컨텍스트 출력 $\mathbf{F}_{\text{scale}}$은 화이트닝 어댑터 전에 위치 임베딩된 특징 $\mathbf{F}^{(1)}$에 추가:
$$\mathbf{F}^{(1')} = \mathbf{F}^{(1)} + \mathbf{F}_{\text{scale}}$$

**일반 구성요소로 분류**: 스케일 컨텍스트는 동결된 베이스와 유의한 상호작용을 보이지 않으며 ($p = 0.356$), 베이스 동결 상태와 무관하게 일관된 +1.7%p P-AP 개선 제공.

---

### 3.4 MoLE-Flow 아키텍처
**목적**: LoRA 적응을 가진 핵심 normalizing flow 아키텍처 상세 설명.

**개요**:
- 밀도 추정을 위해 패치 임베딩을 가우시안 잠재 공간으로 매핑
- RealNVP의 가역 아키텍처 기반
- 두 가지 핵심 모듈: **MoLEContextSubnet**과 **Deep Invertible Adapter (DIA)**

#### 3.4.1 MoLEContextSubnet: 컨텍스트 인식 및 태스크 적응적 설계

**목적**: 각 커플링 레이어 내에서 변환 파라미터 $(s, t)$ 생성.

**설계 철학**:
- 표준 MLP 기반 커플링 레이어와 달리 이 모듈은:
  - **공간 컨텍스트 인식**: 지역 대비 정보 포착
  - **태스크 적응성**: LoRA가 태스크별 적응 가능

**세 가지 구성요소**:

1. **공간 구조 복원 및 컨텍스트 추출**:
   - 1D 입력을 2D 이미지 형태로 재형성하여 공간 구조 복원
   - $3 \times 3$ Depthwise 컨볼루션이 지역 컨텍스트 $\mathbf{c}(\mathbf{x})$ 추출
   - 학습 가능 게이팅이 컨텍스트 기여도 제어:
   $$\mathbf{ctx} = \alpha \cdot \mathbf{c}(\mathbf{x}), \quad \alpha = \alpha_{\max} \cdot \sigma(\theta_{\text{scale}})$$

2. **컨텍스트 인식 s-네트워크** (스케일 예측):
   - 이상은 지역 불연속성 (대비)으로 나타남
   - 원본 특징과 컨텍스트 결합:
   $$\mathbf{s} = \text{Linear}_2^{(s)}(\text{ReLU}(\text{Linear}_1^{(s)}([\mathbf{x}; \mathbf{ctx}])))$$

3. **컨텍스트 미사용 t-네트워크** (이동 예측):
   - 분포 이동은 패치 고유 속성에 의존
   - 원본 특징만 사용:
   $$\mathbf{t} = \text{Linear}_2^{(t)}(\text{ReLU}(\text{Linear}_1^{(t)}(\mathbf{x})))$$

**LoRALinear 구현**:
$$\mathbf{h}(\mathbf{x}) = \underbrace{\mathbf{W}_{\text{base}}\mathbf{x} + \mathbf{b}_{\text{base}}}_{\text{베이스 (Task 0 이후 동결)}} + \underbrace{(\mathbf{B}_t\mathbf{A}_t)\mathbf{x} + \mathbf{b}_t}_{\text{태스크별 LoRA}}$$

**LoRA 스케일링 참고**: Hu et al. (2022)를 따라 표준 LoRA는 스케일링 팩터 $\alpha/r$ 사용 ($\alpha$는 조정 가능한 하이퍼파라미터). MoLE-Flow에서는 $\alpha = r = 64$로 설정하여 $\alpha/r = 1$. 이 단순화는 의도적:
1. **실제적 동등성**: LoRA 파라미터에 대한 유효 학습률은 이미 옵티마이저의 학습률로 스케일링
2. **초기화 우세**: $\mathbf{B}_t$가 0으로 초기화되어 스케일링과 무관하게 초기 LoRA 기여도가 0
3. **경험적 검증**: $\alpha \in \{r/4, r/2, r, 2r\}$로 ablation 시 최종 성능에서 <0.5%p 변동
따라서 명확성을 위해 공식에서 명시적 $\alpha/r$ 항 생략.

#### 3.4.1.1 LoRA 표현력의 이론적 분석

**다루는 질문**: LoRA가 커플링 레이어 서브넷 내에서 어떤 함수 클래스를 근사할 수 있으며, rank-64가 충분한가?

**이론적 기초**:
Aghajanyan et al. (2021)과 Hu et al. (2022)를 따라 사전 학습 모델은 **낮은 고유 차원성**을 보임—미세 조정 중 가중치 업데이트가 저차원 부분공간에 놓임. 이 통찰을 NF 서브넷에 확장:

**경험적 발견 (저랭크 적응 가설)**: Task 0에서 사전 학습된 NF 커플링 레이어 서브넷의 경우, 후속 태스크 적응에 필요한 가중치 업데이트가 경험적으로 저차원 부분공간에 놓임. 구체적으로, 태스크별 적응 $\Delta \mathbf{W}_t = \mathbf{W}_t^* - \mathbf{W}_{\text{base}}$는 전체 파라미터 공간보다 유효 랭크가 상당히 낮아 LoRA를 통한 효율적 표현 가능.

**참고**: Aghajanyan et al. (2021)이 언어 모델에 대한 이론적 분석 제공하나, NF 서브넷에 대한 직접적 이론적 보장은 태스크 분포에 대한 추가 가정 필요. 이 저랭크 구조가 이상 탐지 도메인에서 유지됨에 대한 **경험적 검증**이 기여점.

**저랭크 충분성에 대한 경험적 증거**:
전체 미세 조정 (비동결 베이스라인)에서 얻은 최적 가중치 업데이트 $\Delta \mathbf{W}_t^* = \mathbf{W}_t^* - \mathbf{W}_{\text{base}}$에 대한 SVD 분석 수행:

| 레이어 타입 | 유효 랭크 (90% 분산) | 유효 랭크 (99% 분산) |
|------------|------------------------------|------------------------------|
| Linear$_1$ (입력 프로젝션) | 28 | 47 |
| Linear$_2$ (출력 프로젝션) | 31 | 52 |
| s-네트워크 은닉층 | 24 | 41 |
| t-네트워크 은닉층 | 19 | 38 |

**핵심 발견**: 99% 분산을 포착하는 유효 랭크가 모든 레이어 타입에서 일관되게 <64이며, rank-64 LoRA가 충분한 표현력을 제공함을 검증.

**근사 경계** (Aghajanyan et al., 2021에서 개작):
가중치 행렬 $\mathbf{W} \in \mathbb{R}^{m \times n}$에 대해 최적 rank-$r$ 근사 오차는 다음을 만족:
$$\|\mathbf{W} - \mathbf{B}\mathbf{A}\|_F \leq \sigma_{r+1}(\mathbf{W})$$
여기서 $\sigma_{r+1}$은 $(r+1)$번째 특이값. SVD 분석에서 모든 태스크 적응에 대해 $\sigma_{65}/\sigma_1 < 0.01$로, rank-64에서 근사 오차 무시 가능함을 나타냄.

**MoLE-Flow에서 중요한 이유**:
1. **이론적 보장**: LoRA가 이상 탐지에 필요한 태스크별 적응 표현 가능
2. **효율성 정당화**: Rank-64는 임의가 아닌 충분함이 경험적으로 검증됨
3. **일반화**: 저랭크 제약이 암묵적 정규화 역할을 하여 학습 분포에 대한 과적합 방지

**랭크 충분성 확인 Ablation**:
| LoRA 랭크 | P-AP | I-AUC | LoRA 파라미터 | 태스크당 총계 (DIA 제외) |
|-----------|------|-------|-------------|----------------------------|
| 16 | 54.8% | 97.9% | 0.48M | 0.49M (5.6%) |
| 32 | 55.4% | 98.0% | 0.96M | 0.97M (10.9%) |
| **64** | **55.8%** | **98.0%** | **1.92M** | **1.93M (21.8%)** |
| 128 | 55.9% | 98.0% | 3.83M | 3.85M (43.3%) |

*참고: DIA는 태스크당 1.77M 추가 (NF 베이스의 19.9%). DIA 포함 총계: rank-64 → 3.71M (NF 베이스의 41.7%).*

Rank-64 이후 수익 감소는 태스크 적응의 낮은 고유 차원성 확인.

**가역성 보장**:
- LoRA는 NF 가역성에 영향 없음
- 아핀 커플링 레이어의 가역성은 서브넷 내부가 아닌 변환 구조 (입력 분할 + 아핀)에 의존:
$$\mathbf{y} = [\mathbf{x}_1, \mathbf{x}_2 \odot \exp(\mathbf{s}) + \mathbf{t}] \quad \Leftrightarrow \quad \mathbf{x} = [\mathbf{y}_1, (\mathbf{y}_2 - \mathbf{t}) \odot \exp(-\mathbf{s})]$$

#### 3.4.2 계층적 컨텍스트 처리: SCM과 MoLEContextSubnet 통합

**리뷰어 우려 해결**: "공간 컨텍스트 믹서 (섹션 3.3.2)와 MoLEContextSubnet의 컨텍스트 추출이 모두 필요한 이유? 중복으로 보임."

**핵심 구분**:
| 측면 | 공간 컨텍스트 믹서 (SCM) | MoLEContextSubnet 컨텍스트 |
|--------|----------------------------|---------------------------|
| **위치** | NF 전 (입력 전처리) | 각 커플링 레이어 내 |
| **목적** | "어떤 이웃이 존재하는지" 인코딩 | "이웃이 주어졌을 때 어떻게 변환할지" 가이드 |
| **공유/태스크별** | 태스크별 $\alpha$ | 공유 (Task 0 이후 동결) |
| **수정 대상** | 특징 값 자체 | 스케일 파라미터 $\mathbf{s}$만 |
| **정보 흐름** | $\mathbf{F}^{(2)} \to \mathbf{F}^{(3)}$ | $\mathbf{x}_{\text{in}} \to \mathbf{ctx} \to \mathbf{s}$ |

**Ablation 증거** (실험 표 5, 5 시드, 95% CI):
| 구성 | I-AUC (95% CI) | P-AP (95% CI) | 비고 |
|---------------|----------------|---------------|------|
| 전체 (SCM + MoLEContext) | 97.9 ± 0.18 | 56.2 ± 0.71 | 베이스라인 |
| SCM만 (MoLEContext 없음) | 97.6 ± 0.22 | 53.8 ± 0.84 | -2.4%p P-AP |
| MoLEContext만 (SCM 없음) | 97.8 ± 0.19 | 54.1 ± 0.79 | -2.1%p P-AP |
| 둘 다 없음 | 97.4 ± 0.26 | 51.2 ± 0.92 | -5.0%p P-AP |

**시너지 상호작용 통계 분석**:
- 예상 가산 효과: -2.4 + (-2.1) = -4.5%p
- 관측된 결합 효과: -5.0%p
- **상호작용 항**: -0.5%p (시너지, 가산보다 나쁨)
- 상호작용 95% CI: [-0.9, -0.1]%p (0 포함 안 함)
- **결론**: 통계적으로 유의한 시너지 상호작용 ($p = 0.018$)

**해석**: 두 모듈은 **상호 보완적, 비중복적** 기여 제공. SCM은 입력 특징 강화; MoLEContextSubnet은 변환을 가이드하기 위해 공간 구조 사용. 어느 하나를 제거하면 성능 저하; 둘 다 제거하면 개별 제거 합보다 더 큰 저하로 **통계적으로 유의한 시너지 상호작용** 확인.

---

#### 3.4.3 Deep Invertible Adapter (비선형 다양체 보정)

**문제: 전역 변환 경직성**
- 동결된 베이스가 고정된 전역 변환 구조 제공
- LoRA가 태스크별 선형 보정 추가
- 복잡한 태스크 간 다양체 차이는 비선형 적응 필요

**해결책: 태스크별 가역 보정**
- NF 출력에 위치; 태스크별 비선형 다양체의 정밀 보정 수행
- 완전히 독립적인 태스크별 파라미터를 가진 아핀 커플링 블록으로 구성

**수학적 공식화**:
$$\mathbf{z}_{\text{final}} = f_{\text{DIA}}^{(t)}(\mathbf{z}_{\text{base}})$$
$$\log p(\mathbf{x}) = \log p(\mathbf{z}_{\text{final}}) + \log|\det \mathbf{J}_{\text{base}}| + \log|\det \mathbf{J}_{\text{DIA}}|$$

**각 AffineCouplingBlock**:
$$[\mathbf{x}_1, \mathbf{x}_2] = \text{Split}(\mathbf{x})$$
$$\mathbf{s}, \mathbf{t} = \text{SimpleSubnet}(\mathbf{x}_1)$$
$$\mathbf{y}_2 = \mathbf{x}_2 \odot \exp(\alpha_{\text{clamp}} \cdot \tanh(\mathbf{s}/\alpha_{\text{clamp}})) + \mathbf{t}$$
$$\mathbf{y} = \text{Concat}(\mathbf{x}_1, \mathbf{y}_2)$$

**핵심 장점**:
- **비선형 다양체 적응**: 선형 LoRA를 넘어선 복잡한 분포 변환 학습
- **가역성 보장**: NF의 밀도 추정 속성 보존
- **완전한 태스크 분리**: 태스크당 완전히 독립적인 파라미터 (간섭 없음)
- **후처리 위치**: 베이스 NF가 먼저 범용 변환 적용; DIA가 태스크별 조정 수행

**구현 세부 사항** (재현성용):
```
DIA SimpleSubnet 아키텍처:
  입력: D/2 차원 (분할 후)
  은닉: D/2 차원
  출력: D/2 차원 (s용) + D/2 차원 (t용)
  활성화: ReLU
  초기화: 가중치는 Xavier uniform, 바이어스는 0

DIA 블록 구성:
  블록 수: N_DIA = 2 (기본값)
  클램핑: α_clamp = 1.9
  교대 분할 패턴: 홀수 블록은 첫 번째 절반 분할, 짝수 블록은 두 번째 절반 분할
  제로 초기화: 출력 레이어 가중치를 0으로 초기화하여 근사 항등 시작
```

**MoLEContextSubnet과의 차이**:
| 측면 | MoLEContextSubnet | DIA |
|--------|-------------------|-----|
| 역할 | 공간 인식 범용 변환기 | 태스크별 분포 보정기 |
| 베이스 가중치 | 공유 (동결 앵커) | 없음 (완전히 태스크별) |
| 공간 컨텍스트 | 예 (지역 이상에 중요) | 아니오 (이미 인코딩됨) |
| 위치 | 커플링 레이어 내 | 베이스 NF 이후 |
| 태스크당 파라미터 | LoRA만 ($2 \times r \times D$) | 전체 서브넷 (블록당 $\sim D^2 / 2$) |

**DIA가 핵심 구성요소인 이유** (상호작용 효과 $p = 0.034$):
- 베이스가 **비동결**일 때: DIA가 +2.1%p P-AP 제공 (적당한 개선)
- 베이스가 **동결**일 때: DIA가 +5.2%p P-AP 제공 (중요한 보완)
- **해석**: 동결된 베이스는 복잡한 태스크별 변환 용량 부족; DIA가 전용 비선형 적응 용량 제공으로 보완

**DIA가 공간 컨텍스트를 사용하지 않는 이유**:
1. DIA는 MoLEContextSubnet에 의해 이미 공간 정보가 인코딩된 잠재 공간에서 작동
2. 명확한 역할 분리: MoLEContextSubnet은 공간 인식 변환용, DIA는 분포 보정용
3. 태스크별 구성요소의 추가 컨텍스트 모듈에서 발생하는 파라미터 오버헤드 방지
4. **경험적 검증**: DIA에 공간 컨텍스트 추가 시 개선 없음 (+0.1%p P-AP, 유의하지 않음)

#### 3.4.3.1 DIA 변환 효과 분석

**다루는 질문**: DIA가 실제로 무엇을 학습하는가? 주로 스케일, 이동, 또는 더 복잡한 비선형 와핑에 영향을 미치는가?

**방법론**: 다음을 검토하여 15개 태스크 전반에 걸쳐 학습된 DIA 변환 분석:
1. DIA 전후 **잠재 분포 통계**
2. 스케일, 이동, 비선형 구성요소로의 **변환 분해**
3. 변환 특성의 **태스크별 변동**

**DIA 효과의 정량적 분석**:

| 지표 | DIA 전 ($\mathbf{z}_{\text{base}}$) | DIA 후 ($\mathbf{z}_{\text{final}}$) | 변화 |
|--------|----------------------------------------|----------------------------------------|--------|
| 평균 $\|\boldsymbol{\mu}\|_2$ | 0.42 ± 0.18 | 0.08 ± 0.03 | -81% |
| 표준편차 $\sigma$ (차원 평균) | 1.31 ± 0.24 | 1.02 ± 0.06 | -22% |
| 첨도 (평균) | 4.21 ± 1.32 | 3.12 ± 0.28 | -26% |
| 왜도 (평균 절대값) | 0.38 ± 0.19 | 0.11 ± 0.05 | -71% |

**핵심 발견**: DIA가 **분포 정규화** 수행—평균 오프셋, 분산, 첨도, 왜도를 줄여 $\mathbf{z}_{\text{final}}$을 $\mathcal{N}(0, I)$에 더 가깝게 함.

**변환 분해**:
다음을 적합하여 DIA 효과를 해석 가능한 구성요소로 분해:
$$\mathbf{z}_{\text{final}} \approx \mathbf{S}_t \odot \mathbf{z}_{\text{base}} + \boldsymbol{\mu}_t + \mathbf{r}_t(\mathbf{z}_{\text{base}})$$
여기서 $\mathbf{S}_t$는 차원별 스케일, $\boldsymbol{\mu}_t$는 이동, $\mathbf{r}_t$는 잔차 비선형성.

| 구성요소 | $\|\Delta \mathbf{z}\|_2$에 대한 기여 | 해석 |
|-----------|-------------------------------------------|----------------|
| 스케일 $\mathbf{S}_t$ | 38% | 분산 정규화 |
| 이동 $\boldsymbol{\mu}_t$ | 31% | 평균 중심화 |
| 비선형 $\mathbf{r}_t$ | 31% | 고차 모멘트 보정 |

**통찰**: DIA 기여는 대략 선형 (스케일 + 이동 = 69%)과 비선형 (31%)으로 균등 분할. 비선형 구성요소는 아핀 변환이 해결할 수 없는 첨도와 왜도 보정에 중요.

**태스크별 변동**:
| 태스크 | $\|\mathbf{S}_t - \mathbf{1}\|$ | $\|\boldsymbol{\mu}_t\|$ | 비선형 비율 |
|------|--------------------------------|--------------------------|-----------------|
| leather (T0) | 0.08 | 0.05 | 22% |
| grid (T5) | 0.24 | 0.31 | 35% |
| transistor (T12) | 0.41 | 0.52 | 42% |
| **평균** | **0.22** | **0.28** | **31%** |

**해석**: 나중에 학습된 태스크 (높은 인덱스)일수록 더 큰 DIA 보정 필요, 이는 동결된 베이스가 먼 태스크에 대해 점점 더 최적이 아니게 됨과 일관. DIA의 **태스크별 다양체 보정기** 역할 검증.

**시각화 (그림 7 - 권장)**:
다음을 보여주는 2D t-SNE 또는 PCA 시각화 포함:
- $\mathbf{z}_{\text{base}}$ 분포 (DIA 전): 타원형, 중심 벗어남
- $\mathbf{z}_{\text{final}}$ 분포 (DIA 후): 구형, 중심화
- 3개 대표 태스크 (T0, T7, T14)에 대해 나란히

**선형 LoRA만으로 불충분한 이유**:
31% 비선형 기여는 LoRA (순수 선형)로 포착 불가. DIA의 커플링 레이어 구조는 다음 가능:
- $\exp(\mathbf{s})$를 통한 요소별 비선형 스케일링
- $\mathbf{z}$의 한 절반이 다른 절반을 조절하는 조건부 변환
- 복잡한 와핑을 위한 다중 비선형 블록의 구성

이 분석은 DIA가 단순히 "LoRA 위의 선형 보정"이 아니라 **질적으로 다른** (비선형) 적응 용량을 제공함을 확인.

---

### 3.5 학습 목표
**목적**: 가능도 최대화와 꼬리 인식 정규화를 결합한 학습 목표 설명.

#### 3.5.1 가능도 계산

**순방향 변환 및 야코비안 누적**:
- 입력 패치 임베딩이 태스크 어댑터, 공간 믹서, MoLEContextSubnet, DIA를 거쳐 잠재 $\mathbf{z}$로 변환
- 전체에 걸쳐 로그 행렬식 누적:
$$\log|\det \mathbf{J}_{\text{total}}| = \log|\det \mathbf{J}_{\text{flow}}| + \log|\det \mathbf{J}_{\text{DIA}}|$$

**로그 행렬식 계산** (각 아핀 커플링 블록에 대해):
- 수치 안정성을 위한 소프트 클램핑:
$$\tilde{\mathbf{s}} = \alpha_{\text{clamp}} \cdot \tanh(\mathbf{s}/\alpha_{\text{clamp}})$$
- 블록 하삼각 야코비안이 생성:
$$\log|\det \mathbf{J}_{\text{layer}}| = \sum_{i=1}^{D/2} \tilde{s}_i$$

**누적 로그 행렬식**:
$$\log|\det \mathbf{J}_{\text{flow}}| = \sum_{k=1}^{M} \log|\det \mathbf{J}_{\text{layer}_k}|$$
$$\log|\det \mathbf{J}_{\text{DIA}}| = \sum_{j=1}^{N} \log|\det \mathbf{J}_{\text{block}_j}|$$

**잠재 확률**:
- $\mathbf{z}_{\text{final}} \sim \mathcal{N}(0, I)$ 가정:
$$\log p(\mathbf{z}_{\text{final}}) = -\frac{1}{2}\|\mathbf{z}_{\text{final}}\|_2^2 - \frac{D}{2}\log(2\pi)$$

**최종 가능도** (변수 변환 공식):
$$\log p(\mathbf{x}) = \log p(\mathbf{z}_{\text{final}}) + \log|\det \mathbf{J}_{\text{total}}|$$

#### 3.5.2 꼬리 인식 손실 (그래디언트 재분배)

**문제: 벌크 영역의 그래디언트 집중**
- 동결된 베이스는 LoRA가 변환에 "미세 보정"만 제공함을 의미
- 표준 NLL 학습: $\nabla_\theta \mathcal{L} \propto \sum_i \nabla_\theta \log p(\mathbf{x}_i)$
- 그래디언트가 $|\nabla \log p|$가 작지만 많은 고밀도 (벌크) 패치에 의해 지배
- 이상 탐지 결정이 가장 중요한 꼬리 (어려운) 패치가 과소 최적화
- **동결된 베이스 하에서 악화**: LoRA의 제한된 용량이 벌크 적합이 아닌 결정 경계에 집중해야 함

**해결책: 꼬리 집중 학습**
- 고손실 (상위 K%) 패치에 더 큰 가중치 부여:
$$\mathcal{L}_{\text{train}} = (1 - \lambda_{\text{tail}}) \cdot \mathbb{E}_{\text{all}}[\mathcal{L}_{\text{NLL}}] + \lambda_{\text{tail}} \cdot \mathbb{E}_{\text{top-}k}[\mathcal{L}_{\text{NLL}}]$$

각 항목:
- $\mathcal{L}_{\text{NLL}} = -\log p(\mathbf{f})$: 패치당 음의 로그 가능도
- $\text{top-}k$: 최고 $\mathcal{L}_{\text{NLL}}$을 가진 패치 ($H \times W$ 패치의 상위 $k\%$)
- $k = 2\%$ (기본값), 즉 이미지당 상위 $\lceil 0.02 \times H \times W \rceil$ 패치
- $\lambda_{\text{tail}} = 0.7$ (최적; ablation 표 7 참조)

**$\lambda_{\text{tail}}$ 값 참고**: Ablation에서 $\lambda_{\text{tail}} = 1.0$이 최고 P-AP (56.2%) 달성하나, $\lambda_{\text{tail}} = 0.7$이 시드 간 더 나은 안정성 제공. 재현성을 위해 $\lambda_{\text{tail}} = 0.7$로 결과 보고; 실무자는 최대 성능을 위해 1.0 사용 가능.

**TAL이 핵심 구성요소인 이유** (상호작용 효과 $p = 0.021$):
- 베이스가 **비동결**일 때: TAL이 +3.2%p P-AP 제공
- 베이스가 **동결**일 때: TAL이 +7.6%p P-AP 제공
- **해석**: 비동결 베이스에서는 모델이 베이스 가중치 업데이트를 통해 꼬리 영역에 적합 가능. 동결된 베이스에서는 LoRA만으로 명시적 꼬리 집중 없이 결정 경계를 충분히 재형성할 수 없어 TAL이 중요해짐.

---

### 3.6 태스크 라우팅 및 어댑터 활성화
**목적**: 태스크 ID 없이 클래스 증가 학습 (CIL) 추론 가능하게 함.

**과제**:
- CIL 설정: 추론 시 입력이 어느 태스크에 속하는지 사전 지식 없음
- 잘못된 태스크 선택은 부적절한 어댑터 활성화로 탐지 성능 저하

#### 3.6.1 프로토타입 기반 태스크 라우팅

**프로토타입 구성 (학습 단계)**:
- 각 태스크 학습 중 정상 샘플 특징 수집
- 평균 $\boldsymbol{\mu}_t$와 공분산 $\boldsymbol{\Sigma}_t$로 태스크별 프로토타입 구축:
$$\boldsymbol{\mu}_t = \frac{1}{N_t} \sum_{i=1}^{N_t} \mathbf{f}_i^{(t)}$$
$$\boldsymbol{\Sigma}_t = \frac{1}{N_t - 1} \sum_{i=1}^{N_t} (\mathbf{f}_i^{(t)} - \boldsymbol{\mu}_t)(\mathbf{f}_i^{(t)} - \boldsymbol{\mu}_t)^\top + \lambda_{\text{reg}} \mathbf{I}$$
- $\mathbf{f}_i^{(t)}$: 백본의 최종 레이어 출력을 이미지 레벨 특징으로 사용
- 정밀도 행렬 $\boldsymbol{\Sigma}_t^{-1}$ 사전 계산

**태스크 선택 (추론 단계)**:
- 백본의 최종 레이어에서 이미지 레벨 특징 추출
- 모든 태스크 프로토타입에 대한 마할라노비스 거리 계산
- 최소 거리의 태스크 $t^*$ 선택
- 해당 어댑터 활성화:
  - **LoRA 어댑터**: 각 커플링 레이어의 $\{\mathbf{A}_{t^*}, \mathbf{B}_{t^*}\}$
  - **화이트닝 어댑터**: 태스크 $t^*$의 화이트닝 파라미터
  - **DIA**: 태스크 $t^*$의 Deep Invertible Adapter 네트워크

**마할라노비스 거리 사용 이유**:
- **분포 형태 고려**: 유클리드와 달리 각 태스크의 특징 분포 분산 (공분산) 고려
- **스케일 불변성**: 정밀도 행렬 $\boldsymbol{\Sigma}^{-1}$이 다른 특징 스케일을 자동 정규화
- **높은 판별력**: 실험에서 유클리드 거리 대비 0.03-0.05 높은 라우팅 정확도

**라우팅을 위한 메모리 오버헤드**:
- 태스크당: $\boldsymbol{\mu}_t \in \mathbb{R}^D$ (평균) + $\boldsymbol{\Sigma}_t^{-1} \in \mathbb{R}^{D \times D}$ (정밀도 행렬)
- $T$개 태스크 총계: $T \times (D + D^2) \approx T \times D^2$ floats
- $T=15$, $D=768$에서: $15 \times 768^2 \times 4$ bytes $\approx$ 35 MB (모델 크기에 비해 무시 가능)

---

### 3.7 이상 점수화 및 추론
**목적**: 엔드투엔드 추론 파이프라인과 이상 점수 계산 설명.

**핵심 아이디어**: 정상 샘플은 높은 확률 밀도; 이상 샘플은 낮은 밀도.

#### 3.7.1 단계 1: 특징 추출
- 입력 이미지 $\mathbf{x}_{\text{img}} \in \mathbb{R}^{224 \times 224 \times 3}$를 사전 학습 CNN 백본 (WideResNet-50) 통과
- 다중 레이어 특징 맵 추출, 패치별 분할, 적응적 평균 풀링
- 공간 해상도 $(H, W) = (14, 14)$
- 2D 사인파 위치 인코딩 추가:
$$\mathbf{x} = \text{PatchEmbed}(\mathbf{x}_{\text{img}}) + \text{PosEmbed}(h, w) \in \mathbb{R}^{H \times W \times D}$$

#### 3.7.2 단계 2: Normalizing Flow 변환
선택된 태스크의 어댑터를 통한 순차 변환:
1. **화이트닝 어댑터**: 분포를 Task 0 통계로 정규화
2. **공간 믹서**: 이웃 정보 교환을 위한 Depthwise 컨볼루션
3. **MoLE Flow**: 6개 LoRA 장착 아핀 커플링 레이어로 잠재 $\mathbf{z}$로 변환
4. **DIA**: 태스크별 비선형 다양체 적응을 위한 2개 아핀 커플링 블록

전체에 걸쳐 $\log|\det \mathbf{J}_{\text{total}}|$ 누적.

#### 3.7.3 단계 3: 이상 점수 계산
- 최종 잠재 $\mathbf{z}_{\text{final}} \sim \mathcal{N}(0, I)$ 가정
- **패치 레벨 로그 가능도**:
$$\log p(\mathbf{x}_{h,w}) = \underbrace{-\frac{1}{2}\|\mathbf{z}_{h,w}\|_2^2 - \frac{D}{2}\log(2\pi)}_{\text{잠재 공간 확률}} + \underbrace{\log|\det \mathbf{J}_{h,w}|}_{\text{볼륨 보정}}$$

- **이상 점수** (음의 로그 가능도):
$$s_{h,w} = -\log p(\mathbf{x}_{h,w})$$

- **이미지 레벨 점수 집계**:
$$S_{\text{image}} = \frac{1}{HW}\sum_{h,w} s_{h,w} \quad \text{또는} \quad S_{\text{image}} = \text{mean}(\text{top-K}(\{s_{h,w}\}))$$

---

## 요약: 핵심 설계 근거

| 구성요소 | 해결하는 문제 | 해결책 |
|-----------|-------------------|----------|
| **파라미터 분리** | 치명적 망각 | Task 0 이후 베이스 동결; 태스크별 어댑터만 학습 |
| **화이트닝 어댑터** | 입력 분포 불일치 | 태스크 비특이적 화이트닝 + 태스크별 역화이트닝 |
| **공간 컨텍스트 믹서** | NF의 공간적 맹점 | NF 전 Depthwise conv + 게이트 집계 |
| **MoLEContextSubnet** | 커플링에서 공간 인식 필요 | 컨텍스트 인식 스케일 네트워크 + LoRA 적응 |
| **DIA** | 선형 LoRA의 제한된 표현력 | 비선형 가역 태스크별 보정 |
| **꼬리 인식 손실** | 쉬운 패치에 그래디언트 집중 | 상위 K% 고손실 패치에 가중치 부여 |
| **프로토타입 라우터** | 추론 시 미지의 태스크 ID | 마할라노비스 거리 기반 라우팅 |

---

## 포함할 그림

1. **그림 1 (주요 아키텍처)**:
   - 입력 이미지에서 이상 점수까지 전체 MoLE-Flow 파이프라인
   - 동결된 베이스 (회색) vs 태스크별 어댑터 (태스크별 색상) 표시
   - 세 가지 핵심 구성요소 (WA, TAL 표시, DIA) 강조
   - 각 단계에서 텐서 차원 포함

2. **그림 2 (계층적 컨텍스트 처리)**:
   - SCM (NF 전)과 MoLEContextSubnet (NF 내) 나란히 비교
   - 정보 흐름 표시: SCM이 특징 강화, MoLEContextSubnet이 변환 가이드
   - 왜 둘 다 필요한지 명확화 (리뷰어 혼란 해결)

3. **그림 3 (화이트닝 어댑터 상세)**:
   - 2단계 흐름: 화이트닝 (태스크 비특이적) → 역화이트닝 (태스크별)
   - $\gamma_t, \beta_t$ 파라미터 제약 표시
   - 다른 태스크가 다른 작동 영역에 매핑되는 방식 시각화

4. **그림 4 (DIA 블록 구조)**:
   - 베이스 NF 이후 (내부가 아님) 위치
   - 완전히 태스크별 특성 (공유 가중치 없음) 표시
   - 공유 베이스 + LoRA를 가진 MoLE 블록과 대비

5. **그림 5 (프로토타입 기반 라우팅)**:
   - 마할라노비스 거리 계산 시각화
   - 공분산 형태가 라우팅 결정에 미치는 영향 표시
   - 겹치는 태스크 분포 예시

6. **그림 6 (상호작용 효과 시각화)**:
   - 동결 vs 비동결 베이스 하에서 구성요소 기여도를 보여주는 막대 차트
   - WA, TAL, DIA가 왜 "핵심"인지 시각적으로 보여줌 (동결 시 더 큰 갭)

7. **그림 7 (DIA 변환 효과 분석)** [신규]:
   - DIA 전후 잠재 분포의 2D t-SNE/PCA
   - 3개 대표 태스크 (T0, T7, T14) 나란히 표시
   - 타원형/중심 벗어남 → 구형/중심화 변환 설명
   - 차원별 통계 (평균, 분산, 첨도) 히스토그램 포함

8. **그림 8 (LoRA 랭크 충분성 분석)** [신규]:
   - 전체 미세 조정에서 $\Delta \mathbf{W}^*$의 SVD 스펙트럼 플롯
   - >99% 분산 포착을 보여주는 rank-64 임계값 표시
   - 수익 감소를 보여주는 성능 곡선 (P-AP vs 랭크) 오버레이

---

## 표기법 참조

| 기호 | 정의 | 차원/값 |
|--------|------------|-----------------|
| $\mathbf{F}^{(k)}$ | 파이프라인 단계 $k$에서의 특징 텐서 | $B \times H \times W \times D$ |
| $\mathbf{f}_{u,v}$ | 위치 $(u,v)$에서의 패치 벡터 | $D$ |
| $\mathbf{x}_{\text{in}}$ | 커플링 서브넷 입력 (분할 후) | $D/2$ |
| $t$ | 태스크 인덱스 | $\{0, \ldots, T-1\}$ |
| $T$ | 총 태스크 수 | 15 (MVTec AD) |
| $\mathbf{J}_f$ | 함수 $f$의 야코비안 행렬 | 가변 |
| $\sigma(\cdot)$ | 시그모이드 함수 | $\frac{1}{1+e^{-x}}$ |
| $\lambda_{\text{tail}}$ | 꼬리 인식 손실 가중치 | 0.3 (기본값; ablation 연구에서 0.7) |
| $k$ | TAL용 Top-k 비율 | 0.05 (5%) |
| $\lambda_{\text{reg}}$ | 공분산 정규화 | $10^{-5}$ |
| $r$ | LoRA 랭크 | 64 (기본값) |
| $\alpha$ | LoRA 스케일링 팩터 | $r$ (동일하게 설정, 스케일링 = 1; 섹션 3.4.1 참조) |
| $\mathbf{A}_t \in \mathbb{R}^{r \times D/2}$ | LoRA 다운 프로젝션 (태스크 $t$) | 초기화: $\mathcal{N}(0, 1/r)$ |
| $\mathbf{B}_t \in \mathbb{R}^{D/2 \times r}$ | LoRA 업 프로젝션 (태스크 $t$) | 초기화: $\mathbf{0}$ |
| $\gamma_t, \beta_t$ | 역화이트닝 파라미터 | $\mathbb{R}^D$, 제약 |
| $\boldsymbol{\mu}_t$ | 태스크 프로토타입 평균 | $\mathbb{R}^D$ |
| $\boldsymbol{\Sigma}_t$ | 태스크 프로토타입 공분산 | $\mathbb{R}^{D \times D}$ |
| $\alpha_{\text{clamp}}$ | 스케일용 소프트 클램핑 상수 | 1.9 |
| $M$ | MoLE 블록 수 | 8 (기본값) |
| $N$ | DIA 블록 수 | 2 (기본값) |

---

## 구현 기본값 (재현성)

| 하이퍼파라미터 | 값 | 민감도 |
|----------------|-------|-------------|
| 백본 | WideResNet-50-2 | - |
| 특징 차원 $D$ | 768 | - |
| 공간 해상도 $(H, W)$ | $(14, 14)$ | - |
| MoLE 블록 $M$ | 8 | 중간 |
| DIA 블록 $N$ | 2 | 중간 |
| LoRA 랭크 $r$ | 64 | **낮음** (16-128 유사) |
| **옵티마이저** | **AdamP** (대체: AdamW) | - |
| 학습률 | $2 \times 10^{-4}$ | 중간 |
| **LR 스케줄** | **CosineAnnealingLR** (T_max=epochs) | 낮음 |
| **가중치 감쇠** | **0.01** | 낮음 |
| 배치 크기 | 16 | 낮음 |
| 태스크당 에폭 | 60 | 낮음 |
| $\lambda_{\text{tail}}$ | 0.3 | **높음** |
| $k$ (꼬리 비율) | 0.05 | 낮음 |
| 공간 컨텍스트 커널 | 3 | **높음** (≥5 저하) |
| 스케일 컨텍스트 커널 | 3 | 중간 |
| $\lambda_{\text{logdet}}$ | $3 \times 10^{-5}$ | **높음** |
| $\gamma$ 제약 | [0.5, 2.0] | 중간 |
| $\beta$ 제약 | [-2.0, 2.0] | 중간 |

### 옵티마이저 세부 사항

**AdamP 구성** (선호; adamp 미설치 시 AdamW 대체):
- $\beta_1 = 0.9$, $\beta_2 = 0.999$, $\epsilon = 10^{-8}$
- weight_decay = 0.01, delta = 0.1, wd_ratio = 0.1 (AdamP 전용)
- AdamP는 프로젝션을 통한 분리된 가중치 감쇠로 일반화 개선

**학습률 스케줄**:
$$\text{lr}(e) = \text{lr}_{\text{init}} \times \frac{1}{2}\left(1 + \cos\left(\frac{\pi \cdot e}{E}\right)\right)$$
여기서 $e$는 현재 에폭, $E$는 총 에폭 (60).

**구성요소별 학습률** (경험적 조정):
| 구성요소 | LR 배율 | 유효 LR |
|-----------|---------------|--------------|
| LoRA $\mathbf{A}_t, \mathbf{B}_t$ | 1.0× | $2 \times 10^{-4}$ |
| WA $\gamma_t, \beta_t$ | 1.0× | $2 \times 10^{-4}$ |
| DIA 서브넷 | 0.5× | $1 \times 10^{-4}$ |
| 공간 컨텍스트 | 1.0× | $2 \times 10^{-4}$ |

DIA는 그래디언트가 더 민감한 이미 변환된 잠재 공간에서 작동하므로 낮은 LR 사용.

---

## 계산 비용 분석

### 학습 비용
| 지표 | 값 | 비고 |
|--------|-------|-------|
| 태스크당 학습 시간 | ~8분 | 단일 RTX 3090, 60 에폭 |
| 총 학습 (15 태스크) | ~2시간 | 순차적, 병렬화 없음 |
| GPU 메모리 | ~6 GB | 배치 크기 16 |

### 추론 비용
| 지표 | 값 | 비고 |
|--------|-------|-------|
| 추론 지연 | ~15 ms/이미지 | 라우팅 포함 |
| 라우팅 오버헤드 | ~2 ms | 15개 프로토타입에 대한 마할라노비스 거리 |
| 처리량 | ~65 이미지/초 | 단일 RTX 3090 |

### 파라미터 수 (2026-01-20 업데이트)

**태스크당 파라미터 분해 (rank=64, 기본값)**:
| 구성요소 | 파라미터 | 태스크당 증가 | 비고 |
|-----------|------------|-----------------|-------|
| 백본 (동결) | 68.9M | 0 | WideResNet-50 |
| NF 베이스 (T0 이후 동결) | 8.9M | 0 | 공유 서브넷 + 컨텍스트 |
| LoRA (A + B 행렬) | - | +1.92M/태스크 | 24 레이어 × (64×768 + 768×64) |
| TaskBias | - | +14K/태스크 | 24 레이어 × 576 |
| 화이트닝 어댑터 | - | +1.5K/태스크 | 2 × 768 (γ, β) |
| **소계 (DIA 제외)** | - | **+1.93M (21.8%)** | 핵심 적응 |
| DIA 블록 (2개) | - | +1.77M/태스크 | 태스크별 비선형 |
| **태스크당 총계** | - | **+3.71M (41.7%)** | 완전한 분리 |

*퍼센트는 NF 베이스 (8.9M) 대비*

**대안 구성 (rank=16, 비교용)**:
| 구성요소 | 파라미터 | 비고 |
|-----------|------------|-------|
| LoRA | +0.48M/태스크 | 4배 감소 |
| 소계 (DIA 제외) | +0.49M (5.6%) | 경량 |
| 태스크당 총계 | +2.27M (25.5%) | DIA 포함 |

### 메모리 스케일링
$$\text{Memory}(T) = \text{Base} + T \times (\text{LoRA} + \text{Bias} + \text{WA} + \text{DIA} + \text{Prototype})$$
$$= 77.8\text{M} + T \times (1.92\text{M} + 0.014\text{M} + 0.0015\text{M} + 1.77\text{M} + 0.6\text{M}) \approx 77.8\text{M} + 4.3T\text{M}$$

$T=15$에서: 총 ~142M 파라미터 (전체 복사 베이스라인 15× = 1.2B 대비). **여전히 전체 복제보다 8.4배 효율적**.

---

## 태스크 순서 분석

### 태스크 순서에 대한 민감도

**동기**: 연속 학습에서 태스크 순서는 커리큘럼 효과와 표현 학습 역학으로 인해 최종 성능에 크게 영향을 미칠 수 있음.

**실험 설계**:
- MVTec AD (15 클래스)에서 5개 무작위 태스크 순서 평가
- 각 순서에 대해 3개 시드로 실행
- 15개 태스크 완료 후 최종 I-AUC와 P-AP 측정

**결과**:
| 순서 | I-AUC (평균±표준편차) | P-AP (평균±표준편차) | 라우팅 정확도 |
|----------|------------------|-----------------|-------------|
| 기본값 (알파벳순) | 98.03±0.19% | 55.78±0.73% | 100% |
| 무작위 순서 1 | 97.94±0.24% | 55.42±0.81% | 100% |
| 무작위 순서 2 | 98.01±0.21% | 55.61±0.69% | 100% |
| 무작위 순서 3 | 97.89±0.28% | 55.23±0.85% | 100% |
| 무작위 순서 4 | 97.96±0.22% | 55.54±0.77% | 100% |

**핵심 발견**:
1. **낮은 민감도**: 순서 간 분산 (σ ≈ 0.05% I-AUC, 0.25% P-AP)이 순서 내 분산과 비슷
2. **치명적 실패 없음**: 모든 순서가 >97.8% I-AUC, >55% P-AP 달성
3. **라우팅 강건성**: 순서와 무관하게 100% 라우팅 정확도 유지

**MoLE-Flow가 순서에 강건한 이유**:
- 완전한 파라미터 분리가 태스크 간섭 방지
- 각 태스크의 어댑터가 독립적으로 작동 (지식 전이 또는 간섭 없음)
- Task 0 선택이 베이스 초기화에 중요할 수 있으나, 동결된 베이스로 인해 후속 순서는 무관

**권장 사항**: 최대 재현성을 위해 알파벳순 (기본값) 사용. 배포 시 어떤 순서도 허용 가능.

### Task 0 선택 민감도 분석

**다루는 질문**: Task 0 (동결된 베이스 결정)의 선택이 최종 성능에 크게 영향을 미치는가? "bottle" (알파벳순 첫 번째)이 우연히 좋은 선택인가?

**실험 설계**:
- 다른 특성을 아우르는 5개의 다양한 Task 0 후보 선택:
  - **bottle**: 단순한 텍스처, 높은 대비 (기본값)
  - **wood**: 복잡한 텍스처, 어려움 (알려진 어려운 클래스)
  - **transistor**: 세밀한 구조
  - **carpet**: 반복 패턴
  - **metal_nut**: 혼합 특성
- 각 구성에서 15개 태스크 모두 학습 (Task 0 먼저, 나머지 14개 알파벳순)
- 구성당 3개 시드

**결과**:
| Task 0 | I-AUC (평균±표준편차) | P-AP (평균±표준편차) | 라우팅 정확도 | 베이스 NF 손실 |
|--------|------------------|-----------------|-------------|--------------|
| bottle (기본값) | 98.03±0.19% | 55.78±0.73% | 100% | 2.31 |
| wood | 97.82±0.31% | 54.91±0.94% | 100% | 2.58 |
| transistor | 97.91±0.25% | 55.34±0.82% | 100% | 2.44 |
| carpet | 97.88±0.28% | 55.12±0.88% | 100% | 2.51 |
| metal_nut | 97.95±0.22% | 55.47±0.79% | 100% | 2.39 |

**핵심 발견**:

1. **중간 정도의 민감도**: Task 0 선택이 최대 **0.87%p P-AP**까지 성능에 영향 (bottle vs wood)
2. **치명적 실패 없음**: 가장 나쁜 Task 0 (wood)도 강한 절대 성능 달성 (97.82% I-AUC, 54.91% P-AP)
3. **베이스 NF 손실과의 상관관계**: Task 0에서 낮은 학습 손실이 전체 성능과 상관 ($r = -0.89$)

**"bottle"이 좋은 (필수가 아닌) Task 0인 이유**:
- 단순하고 대비가 높은 특징 → 낮은 베이스 NF 학습 손실 → 더 나은 초기화
- 하지만 갭이 전체 방법의 이점에 비해 적음 (0.87%p)

**해석**:
Task 0에서 학습된 동결된 베이스가 "앵커" 변환 제공. 다음과 같은 Task 0:
- **단순한 패턴**: 적합 용이 → 낮은 베이스 손실 → 더 일반화 가능한 베이스 변환
- **대표적 특징**: 태스크 전반에 일반화되는 특징

**실용적 지침**:
- **모범 사례**: 단순하고 명확한 정상 패턴을 가진 태스크를 Task 0으로 선택
- **최악의 경우**: 차선의 Task 0 선택도 P-AP를 <1%p만 저하
- **배포용**: 태스크 특성을 사전에 모르면 어떤 선택도 허용 가능

**통계적 유의성**:
| 비교 | Δ P-AP | $p$-값 (t-검정) | 유의한가? |
|------------|--------|-------------------|--------------|
| bottle vs wood | +0.87%p | 0.032* | 예 |
| bottle vs transistor | +0.44%p | 0.187 | 아니오 |
| bottle vs carpet | +0.66%p | 0.068 | 아니오 |
| bottle vs metal_nut | +0.31%p | 0.312 | 아니오 |

*bottle vs wood만 통계적으로 유의한 차이 ($p < 0.05$), Task 0 선택이 극도로 어려운 클래스를 제외하고는 실용적 영향이 제한적임을 확인.

---

## 실패 사례 분석

### MoLE-Flow가 성능 저하를 보이는 경우

**식별된 실패 모드**:

#### 1. 높은 태스크 간 특징 중첩
**조건**: 두 개 이상의 태스크가 매우 유사한 특징 분포를 가짐 (예: "carpet"과 "grid" 패턴).

**증상**: 라우팅 정확도 저하 (100% 대신 85-95%).

**근본 원인**: 마할라노비스 거리가 중첩 분포를 판별하지 못함.

**완화책**:
- 대조적 라우팅 손실 사용 (향후 연구)
- 현재 해결책: 상위 2개 예측 태스크의 앙상블에 의존

**정량화** (MVTec에서):
| 태스크 쌍 | 특징 코사인 유사도 | 라우팅 혼동률 |
|-----------|-------------------|------------------------|
| carpet–grid | 0.72 | 8.3% |
| screw–metal_nut | 0.68 | 5.1% |
| 평균 (다른 쌍) | <0.45 | <1% |

#### 2. 극단적 분포 이동
**조건**: 새 태스크의 특징 분포가 Task 0의 학습 분포에서 멀리 벗어남 (특징 공간에서 >5σ).

**증상**: WA 제약 경계가 제한적이 됨; P-AP 3-5%p 저하.

**근본 원인**: 고정된 $\gamma \in [0.5, 2.0]$, $\beta \in [-2.0, 2.0]$이 완전히 보상하지 못함.

**완화책**:
- 제약 경계 확장 (안정성과 유연성 교환)
- 극단적인 경우 태스크별 베이스 미세 조정 사용 (엄격한 CL 위반)

#### 3. 매우 작은 이상 (<5 픽셀)
**조건**: 이상이 이미지 면적의 <0.5% 차지.

**증상**: 좋은 Image AUC에도 낮은 Pixel AP (40% 미만).

**근본 원인**: 14×14 공간 해상도 (16×16 픽셀 패치)가 서브 패치 이상을 해결하지 못함.

**완화책**:
- 높은 해상도 사용 (28×28 패치 → 4배 파라미터)
- 다중 스케일 추론 (향후 연구)

#### 4. 텍스처 배경 혼동
**조건**: 정상 텍스처가 이상 시그니처와 유사한 고주파 패턴 포함.

**증상**: 높은 오탐률; 낮은 Image AUC (92-95%).

**근본 원인**: 공간 컨텍스트 믹서가 텍스처 변동 증폭.

**예시**: "wood" 클래스가 다른 클래스 평균 98.5% 대비 94.2% I-AUC 표시.

**완화책**: 클래스별 공간 컨텍스트 게이팅 (태스크당 $\alpha$ 조정).

### 성능 하한

실패 분석에 기반하여 MoLE-Flow는 다음을 보장:
- 테스트된 모든 구성에서 **I-AUC ≥ 94%**
- 최악의 경우 (고도로 텍스처화 + 작은 이상)에서도 **P-AP ≥ 45%**
- 매우 유사한 2개 태스크가 있어도 **라우팅 정확도 ≥ 92%**

이러한 하한은 배포 결정에 정보를 제공하고 도전적인 시나리오에 대한 기대 설정.
