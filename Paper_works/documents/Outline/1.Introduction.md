	Introduction
	배경 : Continual AD의 필요성 
	딥러닝 기반 이상 탐지(AD)는 정상 데이터의 분포만을 학습하는 단일 클래스 분류(One-Class Classification)에서 시작하여 최근 다중 클래스로 확장되었으나, 이는 학습 시점에 모든 데이터가 수집된 정적인 환경(Static Environment)을 가정한다는 한계가 있음 
	실제 제조 현장은 새로운 제품 클래스가 순차적으로 등장하는 역동적인 특성을 가지며, 이는 정적인 학습 가정과 배치됨
	과거의 모든 데이터를 축적하여 재학습하는 것은 기하급수적으로 증가하는 저장 비용뿐만 아니라, 프라이버시 제약 등으로 인해 현실적으로 불가능함
	결국 모델은 이전 데이터를 볼 수 없는 상태에서 새로운 태스크를 학습해야 하며, 이 과정에서 이전에 학습한 지식을 급격히 소실하여 시스템의 신뢰성을 무너뜨리는 파멸적 망각 문제에 직면함 
	기존 방법론들의 한계 
	결정 경계만 조정하면 되는 분류 모델과 달리, 이상 탐지, 특히 Normalizing Flow 기반의 방법론은 정상 데이터의 확률 밀도 자체를 정밀하게 추정해야 함 
	따라서 미세한 파라미터 간섭만으로도 우도 매니폴드가 붕괴되어 치명적인 성능 저하를 초래하므로, AD에서는 망각을 단순히 ‘완화’하는 것에는 한계가 있음 
	많은 SOTA 방법들[CADIC, ReplayCAD 등]이 채택하고 있는 Replay 방식은 Task 수에 비례하여 데이터 메모리 비용이 증가하여 확장성을 허재함. 또한 제한된 버퍼 크기로는 고차원 데이터의 복잡한 꼬리 분포(Tail Distribution)를 커버할 수 없어, 결국 편향된 밀도 추정을 야기함 
	따라서 구조적으로 파라미터를 분리하여 forgetting을 피하기 위한 Parameter isolation 방법들이 시도 됨 [SurpriseNet, Continual-MEGA] 
	이러한 방법론들은 데이터 의존성을 파라미터를 물리적으로 분리하는 방식을 차용하며, Task별로 독립된 네트워크를 할당함에 따라 모델 크기의 선형적 증가를 초래하거나, 고정된 용량을 분할하는 경우 가용 파라미터가 고갈되는 용량 포화 문제에 직면
	모델 비용을 줄이기 위해 feature space에 prompt 또는 adapter [UCAD,DER,TAPD]를 적용하는 시도가 있었으나, 적절한 프롬프트 선택을 위해 여전히 별도의 메모리 뱅크나 라우팅을 위한 추가적인 프로세스가 요구되어 완전한 효율성을 달성하지 못 함 

	The Isolation-Efficiency Trade-off 
	기존 Continual AD 연구들은 Isolation(망각 방지)과 Efficiency(비용 효율) 간의 trade-off에 직면해 있음
	성능을 위해 완전 격리 시 모델 비용이 급증하며, 반면 효율적 격리를 할 시 정밀도 저하 및 여전히 보조 비용이 발생함 
	본 연구는 이 딜레마를 타개하기 위해, 데이터 저장 없이(No Replay) 그리고 모델의 폭발적 증가 없이(Parameter Efficient) NF의 구조적 특성을 활용한 새로운 해법을 제안
	Key Insight : Theoretical and Empirical Foundations 
	Insight 1 : Structural Suitability of Normalizing Flows
	“전체 모델을 복사할 필요 없이, 변화가 필요한 부분만 분리하면 된다.”는 아이디어에 착안하여, 파라미터를 공유되는 부분(w_shared)과 Task별 변화량(ΔW_task)으로 분해하는 전략을 채택 w_task=w_shared+Δw_task
	전체 모델을 복사할 필요 없이, 변화가 필요한 부분만 분리하면 된다"는 아이디어를 실현하기 위해서는, 파라미터 분해가 모델의 수학적 정합성을 해치지 않아야 함
	Memory Bank 기반의 방법론들은 메모리 자체가 리플레이 문제로 직결이 됨. 
	Reconstruction 기반의 방법론들의 경우 인코더와 디코더가 강하게 결합되어 있어, 파라미터 분해 시 Latent Consistency가 붕괴되어 재구성이 불가능함 
	Teacher-Student : teacher와 Student 간의 정교한 특징 정렬이 요구되므로, 분해 시 학습 불안정성을 초래함 
	NF의 구조적 적합성 (Arbitrary Function Property)
	반면, NF의 Affine Coupling Layer)**는 $y_2 = x_2 \odot \exp(s(x_1)) + t(x_1)$ 연산에서 스케일($s$)과 이동($t$) 함수가 어떤 복잡한 형태이든 관계없이 전체 변환의 가역성(Invertibility)과 야코비안 계산의 효율성을 구조적으로 보장
	우리는 이 Arbitrary Function Property를 효율적 격리를 위한 구조적 안전장치'로 재해석했습니다. 즉, 서브넷을 $[Frozen Base + \Delta W]$ 형태로 분해하더라도 이론적 정당성이 완벽히 유지됨을 발견
	즉 Normalizing Flow의 Coupling layer는 서브넷의 구현 방식과 무관하게 전체 변환의 가역성을 구조적으로 보장하므로 분해에 가장 적합한 모델이라 할 수 있음 
	Insight 2 : Intrinsic Low-Rank Nature of Adaptation (Empirical Insight)
	구조적으로 분해가 가능하더라도, 변화량 $\Delta W$가 커야 한다면 효율성을 달성할 수 없습니다. 우리는 "과연 $\Delta W$는 얼마나 커야 하는가?"에 대한 답을 찾고자 함
	ImageNet으로 사전 학습된 Backbone이 이미 유의미한 특징(Task-agnostic Representation)을 추출한다는 점에 주목
	NF의 역할은 새로운 특징을 학습하는 것이 아니라, 이미 주어진 공간을 가우시안분포로 매핑하는 구조적 변환에 국한 
	이 변환의 기본 골격은 Task간 공유 가능하며, Task별 차이는 Low-Rank 만으로도 충분할 것임 
	실제 학습 시 가중치 변화량에 대해 특이값 분해를 수행한 결과 전체 에너지의 74% 이상이 상위 64개의 랭크에 집중됨을 확인 
	또한 rank ablation 결과 rank 16 ~ 128에서 0.02%로 성능 차이가 미미하며, Task 적응이 본질적으로 저차원임을 확인함 
	Proposed Method : DecoFlow 
	Coupling-level Decomposition 
	DecoFlow는 NF의 Coupling-level에서 서브넷 내부의 변환 함수(Transformation)만을 변경하는 접근을 취함. 아핀 커플링 레이어의 스케일(s)과 이동(t) 〖s(x)=s_base (x)+Δs_task (x),(where Δs=B⋅A)〗_ 형태로 분해하여 적용
	s_base (x)+Δs_task (x) 또한 여전히 하나의 함수이므로, NF의 Arbitrary Function Property에 의해 전체 변환의 가역성과 야코비안 계산의 효율성이 수학적으로 보존 
	Coupling subnet을 Frozen Base와 Task-specific LoRA로 이원화 하여 구성함. Task 0 이후 base weight를 동결하여 모든 Task가 공유하는 구조적 앵커로 삼고, 이후 task는 저랭크의 LoRA 모듈만 학습. 
	이를 통해 s_base의 물리적 보존을 통한 완전한 격리를 달성함 (per-task overhead: 22% of NF base, excl. DIA; 42% incl. DIA)
	4) Integral Components for Structural Compensation:
	Base Freeze와 Low-rank 구조를 통해 안정성과 효율성을 모두 확보했으나, 고정된 베이스의 경직성, 저차원 적응의 표현력 한계, 전역적 분포 차이, 미세한 꼬리 분포를 포착하는 데에는 여전히 미세한 구조적 간극이 존재
	이를 보강하기 위해 GDA(분포 정렬), TAL(꼬리 학습), ACL(국소 표현력 보강)을 도입함
	Contribution
	Normalizing Flow의 Arbitrary Function Property를 통해 파라미터 분해가 가능함을 보임
	DecoFlow Framework: Coupling-level LoRA 적응을 통해 데이터 저장 없이 실질적인 파라미터 격리를 달성하는 NF 기반 Continual AD 프레임워크 제안. **완전한 parameter isolation으로 100% backward compatibility** 달성
	분포 정렬(WA), 꼬리 학습(TAL), 국소 표현력(DIA)을 통합하여 Low-rank 적응의 성능 상한을 3%p 향상
	MVTec AD 15-class sequential learning에서 98.05% I-AUC 달성 (기존 SOTA 대비 +3.35%p). 
