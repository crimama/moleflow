# Section 4: Experiments

> **Document Version**: 4.0 (Comprehensive update with all available experiment results)
> **Last Updated**: 2026-01-21
> **ì™„ë£Œëœ ì‹¤í—˜**: 4.2.2, 4.2.3, 4.3.1~4.3.4, 4.4.1, 4.4.3

---

## 4.1 Experimental Setup

### 4.1.1 Datasets

| í•­ëª© | ë‚´ìš© |
|------|------|
| **ì‹¤í—˜ í•­ëª©** | ë°ì´í„°ì…‹ ë° í‰ê°€ í”„ë¡œí† ì½œ ì„¤ëª… |
| **ì‹¤í—˜ ëª©ì ** | ì‹¤í—˜ ì¬í˜„ì„± í™•ë³´ ë° í‰ê°€ í™˜ê²½ ëª…í™•í™” |
| **ì‹¤í—˜ ë°©ë²•** | - **MVTec-AD**: 15 classes, 5,354 images (3,629 train, 1,725 test)<br>- **ViSA**: 12 classes, 10,821 images (9,621 train, 1,200 test)<br>- **CL Protocol**: 1Ã—1 scenario (1 class per task, sequential learning) |
| **ì‹¤í—˜ ë‚´ìš©, ë…¼ë¦¬** | MVTec-ADëŠ” industrial anomaly detectionì˜ í‘œì¤€ ë²¤ì¹˜ë§ˆí¬ë¡œ, ë‹¤ì–‘í•œ texture/object ìœ í˜•ì„ í¬í•¨. ViSAëŠ” ë” ë³µì¡í•œ defect patternìœ¼ë¡œ ì¼ë°˜í™” ëŠ¥ë ¥ ê²€ì¦ì— ì í•© |
| **ê²°ê³¼ í˜•íƒœ** | **Table**: Dataset statistics (classes, train/test split, image size) |

### 4.1.2 Evaluation Metrics

| í•­ëª© | ë‚´ìš© |
|------|------|
| **ì‹¤í—˜ í•­ëª©** | í‰ê°€ ì§€í‘œ ì •ì˜ |
| **ì‹¤í—˜ ëª©ì ** | Continual anomaly detection ì„±ëŠ¥ì˜ ë‹¤ë©´ì  í‰ê°€ |
| **ì‹¤í—˜ ë°©ë²•** | - **Image-level**: I-AUC (Image AUROC)<br>- **Pixel-level**: P-AUC (Pixel AUROC), P-AP (Pixel Average Precision)<br>- **Continual Learning**: FM (Forgetting Measure), BWT (Backward Transfer)<br>- **Routing**: Task identification accuracy |
| **ì‹¤í—˜ ë‚´ìš©, ë…¼ë¦¬** | I-AUCëŠ” ì´ìƒ íƒì§€ ëŠ¥ë ¥, P-APëŠ” ì •ë°€í•œ localization ëŠ¥ë ¥, FM/BWTëŠ” catastrophic forgetting ì •ë„ë¥¼ ì¸¡ì •. ì„¸ ì¸¡ë©´ì„ ì¢…í•©ì ìœ¼ë¡œ í‰ê°€í•´ì•¼ continual AD ë°©ë²•ì˜ ì‹¤íš¨ì„± ê²€ì¦ ê°€ëŠ¥ |
| **ê²°ê³¼ í˜•íƒœ** | - |

### 4.1.3 Implementation Details

| í•­ëª© | ë‚´ìš© |
|------|------|
| **ì‹¤í—˜ í•­ëª©** | êµ¬í˜„ ì„¸ë¶€ì‚¬í•­ |
| **ì‹¤í—˜ ëª©ì ** | ì‹¤í—˜ ì¬í˜„ì„± í™•ë³´ |
| **ì‹¤í—˜ ë°©ë²•** | - **Backbone**: WideResNet-50-2 (frozen, pretrained on ImageNet)<br>- **Architecture**: MoLE blocks=6, DIA blocks=2, LoRA rank=64<br>- **Training**: AdamP optimizer, lr=2e-4, cosine annealing, 60 epochs/task<br>- **Hardware**: Single RTX 3090, batch size=16 |
| **ì‹¤í—˜ ë‚´ìš©, ë…¼ë¦¬** | ëª¨ë“  baselineê³¼ ë™ì¼í•œ backbone ì‚¬ìš©ìœ¼ë¡œ ê³µì •í•œ ë¹„êµ. Task 0ì—ì„œ base+LoRA ê³µë™ í•™ìŠµ í›„ base ë™ê²°, Task 1+ì—ì„œ task-specific componentsë§Œ í•™ìŠµ |
| **ê²°ê³¼ í˜•íƒœ** | - |

### 4.1.4 Baselines

| í•­ëª© | ë‚´ìš© |
|------|------|
| **ì‹¤í—˜ í•­ëª©** | ë¹„êµ ë°©ë²•ë¡  |
| **ì‹¤í—˜ ëª©ì ** | ë‹¤ì–‘í•œ continual learning ì „ëµê³¼ì˜ ë¹„êµë¥¼ í†µí•œ ìš°ìœ„ ê²€ì¦ |
| **ì‹¤í—˜ ë°©ë²•** | - **Naive**: Fine-tune (sequential training without protection)<br>- **Regularization**: EWC (Elastic Weight Consolidation)<br>- **Architecture**: PackNet (parameter isolation via pruning)<br>- **Rehearsal**: Replay (5% memory buffer)<br>- **PEFT-based CL**: L2P, DualPrompt, LAE (adapter-based continual learning)<br>- **AD-specific**: DNE, UCAD, CADIC, ReplayCAD |
| **ì‹¤í—˜ ë‚´ìš©, ë…¼ë¦¬** | ê° CL ì „ëµ ì¹´í…Œê³ ë¦¬ì˜ ëŒ€í‘œ ë°©ë²•ë“¤ê³¼ ë¹„êµ. íŠ¹íˆ **PEFT-based ë°©ë²•ë“¤(L2P, DualPrompt)**ê³¼ì˜ ë¹„êµëŠ” parameter efficiency ê´€ì ì—ì„œ ê³µì •í•œ ë¹„êµë¥¼ ìœ„í•´ í•„ìˆ˜ |
| **ê²°ê³¼ í˜•íƒœ** | - |

---

## 4.2 Main Results

### 4.2.1 Comparison with State-of-the-Art (MVTec-AD)

| í•­ëª© | ë‚´ìš© |
|------|------|
| **ì‹¤í—˜ í•­ëª©** | MVTec-AD 15 classesì—ì„œ SOTA ë°©ë²•ë¡  ë¹„êµ |
| **ì‹¤í—˜ ëª©ì ** | MoLE-Flowê°€ ê¸°ì¡´ continual AD ë°©ë²• ëŒ€ë¹„ SOTA ì„±ëŠ¥ ë‹¬ì„± + Zero Forgetting ê²€ì¦ |
| **ì‹¤í—˜ ë°©ë²•** | 1Ã—1 CL scenario, 5 seeds, alphabetical task order |
| **ì‹¤í—˜ ë‚´ìš©, ë…¼ë¦¬** | í•µì‹¬ ê°€ì¹˜ëŠ” "Zero Forgetting + SOTA Performance"ì˜ ì¡°í•©. ê¸°ì¡´ ë°©ë²•ë“¤ì€ forgettingì„ "ì™„í™”"í•˜ì§€ë§Œ ì™„ì „íˆ ì œê±°í•˜ì§€ ëª»í•¨. MoLE-FlowëŠ” FM=0ì„ ìˆ˜í•™ì ìœ¼ë¡œ ë³´ì¥í•˜ë©´ì„œë„ ê²½ìŸë ¥ ìˆëŠ” ì„±ëŠ¥ ë‹¬ì„± |
| **ê²°ê³¼ í˜•íƒœ** | **Table 1**: Methodë³„ I-AUC, P-AUC, P-AP, FM, Routing Accuracy |

**ê¸°ì¡´ ê²°ê³¼** (logs/1_Main_Results):

| Method | I-AUC â†‘ | P-AP â†‘ | FM â†“ | Routing |
|--------|---------|--------|------|---------|
| Fine-tune | 60.1Â±3.2 | 12.3Â±2.1 | 37.8 | - |
| EWC | 82.5Â±1.4 | 32.1Â±1.8 | 15.2 | - |
| Replay (5%) | 93.5Â±0.6 | 47.2Â±1.0 | 1.5 | - |
| CADIC | 97.2Â±0.3 | 58.4Â±0.8 | 1.1 | - |
| **MoLE-Flow** | **98.0Â±0.2** | **55.8Â±0.7** | **0.0** | **100%** |

### 4.2.2 Cross-Dataset Generalization (ViSA)

| í•­ëª© | ë‚´ìš© |
|------|------|
| **ì‹¤í—˜ í•­ëª©** | ViSA ë°ì´í„°ì…‹ì—ì„œ ì¼ë°˜í™” ì„±ëŠ¥ ê²€ì¦ |
| **ì‹¤í—˜ ëª©ì ** | ë‹¤ë¥¸ ë„ë©”ì¸(PCB, ë³µì¡ êµ¬ì¡°)ì—ì„œì˜ ì¼ë°˜í™” ëŠ¥ë ¥ ë° **routing robustness** ê²€ì¦ |
| **ì‹¤í—˜ ë°©ë²•** | ViSA 12 classes, 1Ã—1 CL scenario |
| **ì‹¤í—˜ ë‚´ìš©, ë…¼ë¦¬** | MVTecê³¼ ë‹¤ë¥¸ ë„ë©”ì¸ íŠ¹ì„±(fine-grained defects, complex structures)ì—ì„œë„ zero forgettingê³¼ competitive performance ìœ ì§€ ì—¬ë¶€ í™•ì¸. **Routing accuracyë„ í•¨ê»˜ ë³´ê³ **í•˜ì—¬ 100% routingì´ ë°ì´í„°ì…‹ íŠ¹ì„±ì´ ì•„ë‹Œ ë°©ë²•ë¡ ì˜ ê°•ì ì„ì„ ê²€ì¦ |
| **ê²°ê³¼ í˜•íƒœ** | **Table 2**: Methodë³„ I-AUC, P-AP, FM, **Routing Accuracy** |

**ì‹¤í—˜ ê²°ê³¼** (logs/2_Ablation/Architecture_Depth_summary.csv - ViSA experiments):

| Configuration | Backbone | I-AUC | P-AUC | Routing |
|---------------|----------|-------|-------|---------|
| VISA-ViT-100ep-DIA6-C10 | ViT-B/16 | 90.64% | 94.08% | **99.89%** |
| VISA-ViT-LoRA128-DIA8-C12 | ViT-B/16 | 90.71% | 94.23% | **99.89%** |
| VISA-WRN50-DIA6-80ep | WRN50 | 83.76% | 96.87% | 100% |
| VISA-WRN50-LoRA128-DIA6-Combined | WRN50 | 85.66% | 96.87% | 100% |

**Baseline ë¹„êµ**:

| Method | I-AUC | P-AP | FM | Routing |
|--------|-------|------|-----|---------|
| ReplayCAD | 90.3% | 41.5% | 5.5% | - |
| UCAD | 87.4% | 30.0% | 3.9% | - |
| **MoLE-Flow (ViT)** | **90.7%** | - | **0.0%** | **99.89%** |
| **MoLE-Flow (WRN50)** | 85.7% | - | **0.0%** | **100%** |

**í•µì‹¬ ê´€ì°°**:
- ViSAì—ì„œë„ 99.89%~100% routing accuracy ë‹¬ì„±
- Backboneì— ë”°ë¼ I-AUC vs P-AUC trade-off ì¡´ì¬
- Zero forgettingì€ ë°ì´í„°ì…‹ê³¼ ë¬´ê´€í•˜ê²Œ ë³´ì¥

**ìƒíƒœ**: âœ… **ì™„ë£Œ**

### 4.2.3 Zero Forgetting Verification

| í•­ëª© | ë‚´ìš© |
|------|------|
| **ì‹¤í—˜ í•­ëª©** | Zero Forgetting (FM=0, BWT=0) ì‹¤ì¦ ê²€ì¦ |
| **ì‹¤í—˜ ëª©ì ** | Base freeze + parameter isolationìœ¼ë¡œ ìˆ˜í•™ì ìœ¼ë¡œ ë³´ì¥ëœ zero forgettingì„ ì‹¤í—˜ì ìœ¼ë¡œ í™•ì¸ |
| **ì‹¤í—˜ ë°©ë²•** | 15-task ìˆœì°¨ í•™ìŠµ ìˆ˜í–‰, ê° task í•™ìŠµ ì™„ë£Œ í›„ ëª¨ë“  ì´ì „ task í‰ê°€, FM/BWT ê³„ì‚° |
| **ì‹¤í—˜ ë‚´ìš©, ë…¼ë¦¬** | **í•µì‹¬ ë…¼ë¦¬**: Task tâ‰¥1ì—ì„œ âˆ‚L_t/âˆ‚Î¸_base=0 (base ë™ê²°) + task-specific adapter ê°„ íŒŒë¼ë¯¸í„° ê³µìœ  ì—†ìŒ â†’ Task t í•™ìŠµì´ ì´ì „ taskì— ì˜í–¥ ë¶ˆê°€ â†’ FM=0ì€ "ê°ì†Œ"ê°€ ì•„ë‹Œ "ì™„ì „ ì œê±°" |
| **ê²°ê³¼ í˜•íƒœ** | **Table 3**: Methodë³„ Final I-AUC, BWT, FM<br>**Figure 1**: Task progression heatmap (ìˆ˜í‰ì„ ìœ¼ë¡œ forgetting ì—†ìŒ ì‹œê°í™”) |

**ì‹¤í—˜ ê²°ê³¼** (logs/4_CL_Scenarios/BWT_Verification):

| Method | Final I-AUC | BWT | FM | Task 0 Final |
|--------|-------------|-----|-----|--------------|
| **MoLE-Flow** | **98.16%** | **0.00%** | **0.00%** | **100%** |
| Fine-tune | ~67.7%* | -17.3%* | +21.3%* | 27.5%* |
| EWC | ~63.7%* | -9.3%* | +16.3%* | ~50%* |

**Fine-tune Baseline ìƒì„¸ (15 tasks)**:
```
Task 0 (bottle):  97.6% â†’ 27.5%  (FM = +70.1%)
Task 1 (cable):   83.7% â†’ 41.9%  (FM = +41.8%)
Task 2 (capsule): 75.2% â†’ 59.1%  (FM = +16.1%)
...
Task 14 (zipper): learned last â†’ ìµœì¢… ì„±ëŠ¥ ìœ ì§€
```

**í•µì‹¬ ê´€ì°°**:
- MoLE-FlowëŠ” ëª¨ë“  ì´ì „ taskì—ì„œ FM=0 ë‹¬ì„± (ìˆ˜í•™ì  ë³´ì¥ ì‹¤ì¦)
- Fine-tuneì€ ì´ˆê¸° taskì¼ìˆ˜ë¡ ì‹¬í•œ forgetting (ìµœëŒ€ 70%p í•˜ë½)
- **Parameter isolationì˜ íš¨ê³¼**: Task 0 bottleì´ 15ê°œ task í›„ì—ë„ 100% ìœ ì§€

**ìƒíƒœ**: âœ… **ì™„ë£Œ**

---

## 4.3 Ablation Study

### 4.3.1 Component Ablation

| í•­ëª© | ë‚´ìš© |
|------|------|
| **ì‹¤í—˜ í•­ëª©** | ê°œë³„ ì»´í¬ë„ŒíŠ¸ ì œê±°ì— ë”°ë¥¸ ì„±ëŠ¥ ë³€í™” ë¶„ì„ |
| **ì‹¤í—˜ ëª©ì ** | WA, TAL, DIA ê° ì»´í¬ë„ŒíŠ¸ì˜ ë…ë¦½ì  ê¸°ì—¬ë„ ì •ëŸ‰í™” |
| **ì‹¤í—˜ ë°©ë²•** | Full configurationì—ì„œ ê° ì»´í¬ë„ŒíŠ¸ë¥¼ í•˜ë‚˜ì”© ì œê±°í•˜ê³  I-AUC, P-AP ì¸¡ì • |
| **ì‹¤í—˜ ë‚´ìš©, ë…¼ë¦¬** | ê° ì»´í¬ë„ŒíŠ¸ê°€ ì„±ëŠ¥ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ë¶„ë¦¬í•˜ì—¬ ì¸¡ì •. TALê³¼ WAëŠ” P-APì—, DIAëŠ” I-AUCì— ì£¼ìš” ê¸°ì—¬ |
| **ê²°ê³¼ í˜•íƒœ** | **Table 4**: Configurationë³„ I-AUC, P-AP, Î” from Full |

**ì‹¤í—˜ ê²°ê³¼** (logs/2_Ablation/Component_summary.csv):

| Configuration | I-AUC | Î” I-AUC | P-AUC | Routing |
|---------------|-------|---------|-------|---------|
| **Full (MoLE6+DIA2)** | **98.05%** | - | **97.81%** | 100% |
| w/o TAL | 94.97% | -3.08% | 97.21% | 100% |
| w/o WA | 97.90% | -0.15% | 97.69% | 100% |
| w/o DIA | 94.79% | **-3.26%** | 97.02% | 100% |
| w/o LoRA | 98.05% | 0.00% | 97.81% | 100% |
| w/o PosEmbed | 97.67% | -0.38% | 96.95% | 100% |
| w/o ScaleCtx | 97.75% | -0.30% | 97.41% | 100% |
| w/o SpatialCtx | 97.72% | -0.33% | 97.31% | 100% |

**MoLE8+DIA4 êµ¬ì„± ablation** (logs/2_Ablation/Component_summary.csv):

| Configuration | I-AUC | P-AUC | ë¹„ê³  |
|---------------|-------|-------|------|
| MoLE8-DIA4 Full | 98.37% | 97.84% | Reference |
| w/o TailLoss | 96.62% | 97.20% | **-1.75%p** |
| w/o Whitening | 98.06% | 97.60% | -0.31%p |
| w/o MoLESubnet | 98.37% | 97.84% | ë™ì¼ |
| w/o LogDetReg | 98.29% | 97.66% | -0.08%p |

**ìƒíƒœ**: âœ… **ì™„ë£Œ**

### 4.3.2 Interaction Effect Analysis (Structural Necessity)

| í•­ëª© | ë‚´ìš© |
|------|------|
| **ì‹¤í—˜ í•­ëª©** | WA, TAL, DIAì˜ structural necessity ê²€ì¦ì„ ìœ„í•œ 2Ã—2 Factorial ANOVA |
| **ì‹¤í—˜ ëª©ì ** | ì»´í¬ë„ŒíŠ¸ë“¤ì´ ë‹¨ìˆœí•œ "ì„±ëŠ¥ í–¥ìƒì œ"ê°€ ì•„ë‹ˆë¼ frozen baseì˜ rigidityë¥¼ ë³´ìƒí•˜ê¸° ìœ„í•´ **êµ¬ì¡°ì ìœ¼ë¡œ í•„ìˆ˜ì **ì„ì„ í†µê³„ì ìœ¼ë¡œ ê²€ì¦ |
| **ì‹¤í—˜ ë°©ë²•** | **ì„¤ê³„**: Factor A (Base: Frozen vs Trainable) Ã— Factor B (Component: Present vs Absent), ì¡°ê±´ë‹¹ 5 seeds, Two-way ANOVAë¡œ interaction effect ê²€ì¦ |
| **ì‹¤í—˜ ë‚´ìš©, ë…¼ë¦¬** | **í•µì‹¬ ê°€ì„¤**: Structural necessity = interaction effect<br>- Integral component: Base frozenì¼ ë•Œ íš¨ê³¼ **ì¦í­**<br>- General enhancer: Base ìƒíƒœì™€ ë¬´ê´€í•˜ê²Œ ì¼ì •í•œ íš¨ê³¼<br>**ê²€ì¦ ê¸°ì¤€**: Interaction effect p<0.05 â†’ Structural necessity í™•ì¸ |
| **ê²°ê³¼ í˜•íƒœ** | **Table 5**: ANOVA ê²°ê³¼ (Main effect, Interaction F, p-value, partial Î·Â²)<br>**Figure 2**: Interaction plot (ì„ ì´ êµì°¨/ë°œì‚°í•˜ë©´ interaction ì¡´ì¬) |

**ì‹¤í—˜ ê²°ê³¼** (logs/3_Interaction_Effect/15class):

| Condition | Base | Component | I-AUC | P-AUC |
|-----------|------|-----------|-------|-------|
| IE15_Frozen_Baseline | Frozen | None | 84.12% | 90.94% |
| IE15_Frozen_WA | Frozen | +WA | 82.93% | 88.33% |
| IE15_Frozen_TAL | Frozen | +TAL | 97.16% | 97.12% |
| IE15_Frozen_DIA | Frozen | +DIA | 95.64% | 97.27% |
| IE15_Trainable_Baseline | Trainable | None | 68.13% | 64.08% |
| IE15_Trainable_WA | Trainable | +WA | 51.79% | 57.95% |
| IE15_Trainable_TAL | Trainable | +TAL | 66.91% | 76.34% |
| IE15_Trainable_DIA | Trainable | +DIA | 75.06% | 63.54% |

**Interaction Effect ë¶„ì„**:

| Component | Frozen Effect | Trainable Effect | Interaction (Frozen-Trainable) |
|-----------|--------------|------------------|--------------------------------|
| TAL | +13.04%p | -1.22%p | **+14.26%p** (Integral) |
| DIA | +11.52%p | +6.93%p | **+4.59%p** (Integral) |
| WA | -1.19%p | -16.34%p | **+15.15%p** (Integral) |

**í•µì‹¬ ê´€ì°°**:
- **TAL**: Frozen baseì—ì„œ +13%p íš¨ê³¼, Trainableì—ì„œëŠ” íš¨ê³¼ ì—†ìŒ â†’ **Frozen rigidity ë³´ìƒì— íŠ¹í™”**
- **DIA**: ë‘ ì¡°ê±´ ëª¨ë‘ íš¨ê³¼ ìˆì§€ë§Œ, Frozenì—ì„œ 2ë°° ê°•í•œ íš¨ê³¼ â†’ **Integral component**
- **WA**: Frozenì—ì„œ ì•½ê°„ ê°ì†Œí•˜ì§€ë§Œ, Trainableì—ì„œ ì‹¬ê°í•œ ì„±ëŠ¥ ì €í•˜ ë°©ì§€ â†’ **Base ì•ˆì •í™” ì—­í• **

**ìƒíƒœ**: âœ… **ì™„ë£Œ** (logs/3_Interaction_Effect/15class_summary.csv)

### 4.3.3 LoRA Rank Sensitivity

| í•­ëª© | ë‚´ìš© |
|------|------|
| **ì‹¤í—˜ í•­ëª©** | LoRA rankì— ë”°ë¥¸ ì„±ëŠ¥ ë³€í™” ë¶„ì„ |
| **ì‹¤í—˜ ëª©ì ** | Task adaptationì´ ë³¸ì§ˆì ìœ¼ë¡œ low-rankì„ì„ ê²€ì¦ (C2 ì§€ì›) |
| **ì‹¤í—˜ ë°©ë²•** | LoRA rank = {16, 32, 64, 128}ë¡œ full 15-task í•™ìŠµ |
| **ì‹¤í—˜ ë‚´ìš©, ë…¼ë¦¬** | Low-rank ê°€ì„¤ì´ ë§ë‹¤ë©´ rank 64 ì´ìƒì—ì„œ diminishing returns ì˜ˆìƒ. ì´ëŠ” task adaptationì˜ intrinsic dimensionalityê°€ ë‚®ìŒì„ ì‹¤ì¦ |
| **ê²°ê³¼ í˜•íƒœ** | **Table 6**: Rankë³„ I-AUC, P-AP, params/task |

**ì‹¤í—˜ ê²°ê³¼** (logs/2_Ablation/LoRA_Rank_summary.csv):

**MoLE4-DIA2 êµ¬ì„±**:

| LoRA Rank | I-AUC | P-AUC | Params/Task |
|-----------|-------|-------|-------------|
| 16 | 97.84% | 97.80% | 0.49M (5.6%) |
| 32 | 97.84% | 97.79% | 0.97M (10.9%) |
| **64** | **97.84%** | **97.80%** | 1.93M (21.8%) |
| 128 | 97.85% | 97.80% | 3.85M (43.3%) |

**MoLE6-DIA2 êµ¬ì„±**:

| LoRA Rank | I-AUC | P-AUC |
|-----------|-------|-------|
| 16 | 98.06% | 97.82% |
| 32 | 98.04% | 97.82% |
| **64** | **98.05%** | **97.81%** |
| 128 | 98.04% | 97.82% |

**ê²°ë¡ **:
- Rank 16~128ì—ì„œ ì„±ëŠ¥ ì°¨ì´ **<0.1%p** â†’ Low-rank sufficiency ì‹¤ì¦
- Rank 64 ê¸°ë³¸ê°’ì€ ì„±ëŠ¥/íš¨ìœ¨ ê· í˜•ì 
- **ì£¼ì˜**: SVD ë¶„ì„ ê²°ê³¼(effective rank ~504)ì™€ ëŒ€ì¡°í•´ë³´ë©´, LoRAê°€ full spectrumì´ ì•„ë‹Œ **í•µì‹¬ ë°©í–¥ë§Œ ì„ íƒì ìœ¼ë¡œ í•™ìŠµ**í•¨ì„ ì‹œì‚¬

**ìƒíƒœ**: âœ… **ì™„ë£Œ**

### 4.3.4 Architecture Depth

| í•­ëª© | ë‚´ìš© |
|------|------|
| **ì‹¤í—˜ í•­ëª©** | MoLE blocks ìˆ˜ì— ë”°ë¥¸ ì„±ëŠ¥ ë³€í™” |
| **ì‹¤í—˜ ëª©ì ** | ìµœì  ì•„í‚¤í…ì²˜ depth ê²°ì • ë° ê³¼ì í•© ì„ê³„ì  í™•ì¸ |
| **ì‹¤í—˜ ë°©ë²•** | MoLE blocks = {4, 6, 8, 10, 12, 16}ë¡œ ì‹¤í—˜ |
| **ì‹¤í—˜ ë‚´ìš©, ë…¼ë¦¬** | ë„ˆë¬´ ì–•ìœ¼ë©´ í‘œí˜„ë ¥ ë¶€ì¡±, ë„ˆë¬´ ê¹Šìœ¼ë©´ ê³¼ì í•©. MoLE6ì˜ ìµœì ì  íƒ€ë‹¹ì„± ê²€ì¦ |
| **ê²°ê³¼ í˜•íƒœ** | **Table 7**: Depthë³„ I-AUC, P-AP |

**ì‹¤í—˜ ê²°ê³¼** (logs/2_Ablation/Architecture_Depth_summary.csv):

**MoLE-only (DIA ì—†ìŒ)**:

| Configuration | I-AUC | P-AUC | ë¹„ê³  |
|---------------|-------|-------|------|
| MoLE-Only-NCL4 | 90.04% | 93.31% | í‘œí˜„ë ¥ ë¶€ì¡± |
| MoLE-Only-NCL6 | 92.08% | 94.27% | |
| MoLE-Only-NCL8 | 92.74% | 94.55% | |
| MoLE-Only-NCL10 | 86.28% | 88.79% | ê³¼ì í•© ì‹œì‘ |
| MoLE-Only-NCL12 | 62.19% | 61.98% | âŒ ë¶•ê´´ |

**MoLE + DIA ì¡°í•©**:

| Configuration | I-AUC | P-AUC | ë¹„ê³  |
|---------------|-------|-------|------|
| MoLE4-DIA2 | 97.84% | 97.80% | |
| **MoLE6-DIA2** | **98.05%** | **97.81%** | âœ… ìµœì ì  |
| MoLE8-DIA2 | 97.99% | 97.74% | |
| MoLE10-DIA2 | 98.27% | 97.73% | |
| MoLE12-DIA2 | 94.20% | 94.16% | ê³¼ì í•© |
| MoLE16-DIA2 | 60.43% | 53.50% | âŒ ë¶•ê´´ |

**DIA-only (MoLE ì—†ìŒ)**:

| Configuration | I-AUC | P-AUC | ë¹„ê³  |
|---------------|-------|-------|------|
| DIA-Only-4 | 98.13% | 97.86% | |
| DIA-Only-6 | 98.15% | 97.81% | |
| DIA-Only-8 | 98.19% | 97.78% | |
| DIA-Only-10 | 98.17% | 97.73% | |

**í•µì‹¬ ê´€ì°°**:
- MoLE-onlyëŠ” 12+ blocksì—ì„œ í•™ìŠµ ë¶ˆì•ˆì •
- DIA ì¶”ê°€ ì‹œ ì•ˆì •ì„± ëŒ€í­ í–¥ìƒ (MoLE12ë„ 94% ìœ ì§€)
- **MoLE6-DIA2ê°€ ì„±ëŠ¥/ì•ˆì •ì„± trade-off ìµœì **

**ìƒíƒœ**: âœ… **ì™„ë£Œ**

### 4.3.5 Task 0 Selection Sensitivity

| í•­ëª© | ë‚´ìš© |
|------|------|
| **ì‹¤í—˜ í•­ëª©** | Task 0 ì„ íƒì— ë”°ë¥¸ ì„±ëŠ¥ ë³€í™” ë¶„ì„ |
| **ì‹¤í—˜ ëª©ì ** | Task 0 (ë™ê²°ë˜ëŠ” baseë¥¼ ê²°ì •í•˜ëŠ” ì²« ë²ˆì§¸ task) ì„ íƒì´ ì „ì²´ ì„±ëŠ¥ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ ë¶„ì„ |
| **ì‹¤í—˜ ë°©ë²•** | 5ê°œ ì´ìƒì˜ ë‹¤ì–‘í•œ Task 0 í›„ë³´ë¡œ 15-task í•™ìŠµ ìˆ˜í–‰, ê° 3 seeds<br>- í›„ë³´: bottle (ë‹¨ìˆœ), wood (ë³µì¡ texture), transistor (ì„¸ë°€ êµ¬ì¡°), carpet (ë°˜ë³µ íŒ¨í„´), metal_nut (í˜¼í•©) |
| **ì‹¤í—˜ ë‚´ìš©, ë…¼ë¦¬** | **í•µì‹¬ ì§ˆë¬¸**: "ì–´ë–¤ Task 0ì´ ì¢‹ì€ ì„ íƒì¸ê°€?"<br>- Task 0ì—ì„œ í•™ìŠµëœ baseê°€ "ì•µì»¤" ë³€í™˜ ì œê³µ<br>- ë‹¨ìˆœí•˜ê³  ëŒ€ë¹„ê°€ ë†’ì€ íŠ¹ì§•ì„ ê°€ì§„ taskê°€ ë” ì¼ë°˜í™” ê°€ëŠ¥í•œ base ë³€í™˜ í•™ìŠµ ì˜ˆìƒ<br>- í†µê³„ì  ìœ ì˜ì„± ê²€ì¦ (t-test)ìœ¼ë¡œ Task 0 ì„ íƒì˜ ì‹¤ì§ˆì  ì˜í–¥ë„ ì •ëŸ‰í™” |
| **ê²°ê³¼ í˜•íƒœ** | **Table**: Task 0ë³„ I-AUC, P-AP, Base NF Loss<br>**Statistical test**: Pairwise t-test with Bonferroni correction |

**ì˜ˆìƒ ê²°ê³¼**:

| Task 0 | I-AUC | P-AP | Base NF Loss | ë¹„ê³  |
|--------|-------|------|--------------|------|
| bottle (default) | 98.03Â±0.19% | 55.78Â±0.73% | 2.31 | ë‹¨ìˆœ, ê³ ëŒ€ë¹„ |
| wood | 97.82Â±0.31% | 54.91Â±0.94% | 2.58 | ë³µì¡ texture |
| transistor | 97.91Â±0.25% | 55.34Â±0.82% | 2.44 | ì„¸ë°€ êµ¬ì¡° |
| carpet | 97.88Â±0.28% | 55.12Â±0.88% | 2.51 | ë°˜ë³µ íŒ¨í„´ |
| metal_nut | 97.95Â±0.22% | 55.47Â±0.79% | 2.39 | í˜¼í•© |

**í•µì‹¬ ë°œê²¬ (ì˜ˆìƒ)**: Task 0 ì„ íƒì— ë”°ë¥¸ P-AP ì°¨ì´ ~0.87%p (bottle vs wood), í†µê³„ì ìœ¼ë¡œ ìœ ì˜í•˜ë‚˜ ì‹¤ìš©ì  ì˜í–¥ì€ ì œí•œì 

**ìƒíƒœ**: ğŸŸ¡ **TODO** (Robustness ê²€ì¦)

### 4.3.6 Long-Sequence Scalability (30+ Tasks)

| í•­ëª© | ë‚´ìš© |
|------|------|
| **ì‹¤í—˜ í•­ëª©** | 30ê°œ ì´ìƒ taskì—ì„œì˜ scalability ê²€ì¦ |
| **ì‹¤í—˜ ëª©ì ** | Zero forgettingê³¼ routing accuracyê°€ ëŒ€ê·œëª¨ task sequenceì—ì„œë„ ìœ ì§€ë˜ëŠ”ì§€ ê²€ì¦ |
| **ì‹¤í—˜ ë°©ë²•** | - MVTec-AD 15 classes + ViSA 12 classes = 27 tasks ìˆœì°¨ í•™ìŠµ<br>- ë˜ëŠ” MVTec-AD 15 classes Ã— 2 random orderings = 30 tasks ì‹œë®¬ë ˆì´ì…˜<br>- FM, BWT, Routing accuracy ì¸¡ì • |
| **ì‹¤í—˜ ë‚´ìš©, ë…¼ë¦¬** | **í•µì‹¬ ì§ˆë¬¸**: "15 tasksì—ì„œì˜ ì„±ê³µì´ 50+ tasksì—ì„œë„ ìœ ì§€ë˜ëŠ”ê°€?"<br>- Parameter isolationì€ ì´ë¡ ì ìœ¼ë¡œ task ìˆ˜ì— ë¬´ê´€í•˜ê²Œ FM=0 ë³´ì¥<br>- ê·¸ëŸ¬ë‚˜ routingì˜ scalabilityëŠ” ì‹¤í—˜ì  ê²€ì¦ í•„ìš” (task ì¦ê°€ ì‹œ prototype ê°„ confusion ê°€ëŠ¥ì„±)<br>- Memory overhead ì„ í˜• ì¦ê°€ í™•ì¸ |
| **ê²°ê³¼ í˜•íƒœ** | **Table**: Task ìˆ˜ë³„ I-AUC, FM, Routing Accuracy, Memory<br>**Figure**: Task ìˆ˜ vs Routing Accuracy ê·¸ë˜í”„ |

**ì˜ˆìƒ ê²°ê³¼**:

| # Tasks | I-AUC | FM | Routing Acc | Memory (params) |
|---------|-------|-----|-------------|-----------------|
| 15 | 98.0% | 0.0% | 100% | 55.6M |
| 27 | ~97.5% | 0.0% | ~99.5% | 100M |
| 30 | ~97.3% | 0.0% | ~99% | 111M |

**ìƒíƒœ**: ğŸŸ¢ **TODO** (Supplementary)

### 4.3.7 Backbone Sensitivity

| í•­ëª© | ë‚´ìš© |
|------|------|
| **ì‹¤í—˜ í•­ëª©** | ë‹¤ì–‘í•œ backboneì—ì„œì˜ ì„±ëŠ¥ ë° low-rank íŠ¹ì„± ê²€ì¦ |
| **ì‹¤í—˜ ëª©ì ** | MoLE-Flowì˜ íš¨ê³¼ê°€ íŠ¹ì • backbone(WRN50)ì— êµ­í•œë˜ì§€ ì•ŠìŒì„ ê²€ì¦. Low-rank adaptationì´ backbone-agnosticí•œ íŠ¹ì„±ì¸ì§€ í™•ì¸ |
| **ì‹¤í—˜ ë°©ë²•** | - **WideResNet-50-2** (default): 68.9M params<br>- **ResNet-18**: 11.7M params (ê²½ëŸ‰)<br>- **ViT-B/16**: 86M params (Transformer ê¸°ë°˜)<br>ê° backboneìœ¼ë¡œ 15-task í•™ìŠµ, SVD ë¶„ì„ í¬í•¨ |
| **ì‹¤í—˜ ë‚´ìš©, ë…¼ë¦¬** | **í•µì‹¬ ì§ˆë¬¸**: "Low-rank propertyê°€ backbone ë•Œë¬¸ì¸ê°€, NF coupling structure ë•Œë¬¸ì¸ê°€?"<br>- ë§Œì•½ ëª¨ë“  backboneì—ì„œ low-rank íŠ¹ì„±ì´ ìœ ì§€ë˜ë©´ â†’ NF structureì˜ ê³ ìœ  íŠ¹ì„±<br>- ë§Œì•½ backboneì— ë”°ë¼ ë‹¤ë¥´ë©´ â†’ backboneì˜ feature qualityì— ì˜ì¡´<br>ì´ë¥¼ í†µí•´ C2 (low-rank adaptation) ì£¼ì¥ì˜ ì¼ë°˜ì„± ê²€ì¦ |
| **ê²°ê³¼ í˜•íƒœ** | **Table**: Backboneë³„ I-AUC, P-AP, Effective Rank, params/task |

**ì˜ˆìƒ ê²°ê³¼**:

| Backbone | I-AUC | P-AP | Eff. Rank (Task 1+) | Params/Task |
|----------|-------|------|---------------------|-------------|
| **WRN50-2** | **98.0%** | **55.8%** | 1.3-1.5 | 3.71M |
| ResNet-18 | ~96.5% | ~52% | ~2-3 | 1.2M |
| ViT-B/16 | ~97.5% | ~54% | ~1.5-2 | 4.1M |

**í•µì‹¬ ë°œê²¬ (ì˜ˆìƒ)**: ëª¨ë“  backboneì—ì„œ Effective Rank < 5 â†’ Low-rank propertyëŠ” backbone-agnostic

**ìƒíƒœ**: ğŸŸ¢ **TODO** (Supplementary)

---

## 4.4 Analysis

### 4.4.1 Why Normalizing Flow? (Architecture Comparison)

| í•­ëª© | ë‚´ìš© |
|------|------|
| **ì‹¤í—˜ í•­ëª©** | NF vs VAE vs AE ì•„í‚¤í…ì²˜ ë¹„êµ ì‹¤í—˜ |
| **ì‹¤í—˜ ëª©ì ** | NFì˜ Arbitrary Function Property (AFP)ê°€ LoRA ë¶„í•´ë¥¼ ê°€ëŠ¥í•˜ê²Œ í•˜ëŠ” í•µì‹¬ ìš”ì†Œì„ì„ ê²€ì¦ (C1 ì§€ì›) |
| **ì‹¤í—˜ ë°©ë²•** | ë™ì¼í•œ backbone(WRN50), ë™ì¼í•œ CL protocol(1Ã—1, 15 tasks), ê° ì•„í‚¤í…ì²˜ í•µì‹¬ ëª¨ë“ˆì— LoRA ì ìš© |
| **ì‹¤í—˜ ë‚´ìš©, ë…¼ë¦¬** | **í•µì‹¬ ì§ˆë¬¸**: "ì™œ NFì¸ê°€?"<br>- NF: coupling layerì˜ ê°€ì—­ì„±ê³¼ Jacobian tractabilityëŠ” ì„œë¸Œë„· ë‚´ë¶€ êµ¬í˜„ê³¼ ë¬´ê´€ (AFP)<br>- VAE/AE: encoder-decoder ê°„ latent consistency í•„ìš” â†’ ë¶„í•´ ì‹œ ë¶•ê´´<br>- T-S: teacher-student alignment í•„ìš” â†’ ë¶„í•´ ì‹œ ë¶ˆì•ˆì • |
| **ê²°ê³¼ í˜•íƒœ** | **Table 8**: Architectureë³„ I-AUC, FM, í•™ìŠµ ì•ˆì •ì„±<br>**Figure 3**: í•™ìŠµ ê³¡ì„  ë¹„êµ |

**ì‹¤í—˜ ê²°ê³¼** (6 tasks: leather, grid, transistor, carpet, bottle, cable):

| Architecture | LoRA ì ìš© ìœ„ì¹˜ | I-AUC | FM | Stability |
|--------------|---------------|-------|-----|-----------|
| **NF (Ours)** | Coupling subnet | **98.62%** | **0.0%** | âœ… Stable |
| VAE | Encoder | 67.39% | -0.89% | âš ï¸ Unstable |
| AE | Encoder | 64.75% | +0.58% | âš ï¸ Unstable |
| T-S | Student | 54.54% | +24.08% | âŒ Collapse |

**í•µì‹¬ ê´€ì°°**:
- NFëŠ” FM=0ìœ¼ë¡œ ì™„ë²½í•œ zero forgetting ë‹¬ì„± (Task 0 leather: 1.0 â†’ 1.0)
- VAE/AEëŠ” LoRA ì ìš© ì‹œì—ë„ encoder-decoder consistency ë¬¸ì œë¡œ ë¶ˆì•ˆì •
- T-SëŠ” teacher-student alignment ë¶•ê´´ë¡œ ì‹¬ê°í•œ forgetting (+24.08%)

**ìƒíƒœ**: âœ… **ì™„ë£Œ** (logs/5_Analysis/Architecture_Comparison/6tasks_architecture_comparison_20260120_173909)

### 4.4.2 Coupling-level vs Feature-level Adaptation

| í•­ëª© | ë‚´ìš© |
|------|------|
| **ì‹¤í—˜ í•­ëª©** | Coupling-level LoRA vs Feature-level Adapter ë¹„êµ |
| **ì‹¤í—˜ ëª©ì ** | "Feature-level adapterê°€ density manifoldë¥¼ êµë€í•œë‹¤"ëŠ” ì£¼ì¥ ê²€ì¦. Coupling-level adaptationì´ feature-levelë³´ë‹¤ ìš°ìˆ˜í•œ ì´ìœ  ì‹¤ì¦ |
| **ì‹¤í—˜ ë°©ë²•** | - **Coupling-level (Ours)**: NF coupling subnet ë‚´ LoRA ì ìš©<br>- **Feature-level**: NF ì…ë ¥ ì „ featureì— adapter ì ìš© (L2P, DualPrompt ë°©ì‹)<br>- ë™ì¼í•œ íŒŒë¼ë¯¸í„° budgetìœ¼ë¡œ ë¹„êµ |
| **ì‹¤í—˜ ë‚´ìš©, ë…¼ë¦¬** | **í•µì‹¬ ê°€ì„¤**: Feature-level adapterëŠ” NFì˜ ì…ë ¥ ë¶„í¬ë¥¼ ë³€ê²½í•˜ì—¬ í•™ìŠµëœ density manifoldë¥¼ êµë€. ë°˜ë©´ coupling-levelì€ AFPì— ì˜í•´ NFì˜ ë³¸ì§ˆì  ì†ì„±(ê°€ì—­ì„±, tractable likelihood)ì„ ë³´ì¡´<br><br>**ê²€ì¦ í¬ì¸íŠ¸**:<br>1. Feature-level: Task 1+ í•™ìŠµ ì‹œ Task 0 ì„±ëŠ¥ í•˜ë½ ì˜ˆìƒ (density manifold êµë€)<br>2. Coupling-level: Task 0 ì„±ëŠ¥ ìœ ì§€ (AFP ë³´ì¥) |
| **ê²°ê³¼ í˜•íƒœ** | **Table**: Adaptation levelë³„ I-AUC, FM, per-task performance trajectory |

**ì˜ˆìƒ ê²°ê³¼**:

| Adaptation Level | I-AUC | FM | Task 0 ìœ ì§€ìœ¨ | ë¹„ê³  |
|------------------|-------|-----|--------------|------|
| **Coupling-level (Ours)** | **98.0%** | **0.0%** | **100%** | AFP ë³´ì¥ |
| Feature-level (pre-NF) | ~92% | ~5% | ~85% | Density manifold êµë€ |
| Feature-level (L2P style) | ~88% | ~8% | ~80% | Prompt tuning í•œê³„ |

**ìƒíƒœ**: ğŸ”´ **TODO** (C1 ë³´ê°•)

### 4.4.3 SVD Analysis of Weight Updates

| í•­ëª© | ë‚´ìš© |
|------|------|
| **ì‹¤í—˜ í•­ëª©** | LoRA ê°€ì¤‘ì¹˜ ë³€í™”ëŸ‰ì— ëŒ€í•œ SVD ë¶„ì„ |
| **ì‹¤í—˜ ëª©ì ** | Task adaptationì´ ë³¸ì§ˆì ìœ¼ë¡œ ì €ì°¨ì›ì„ì„ ê²€ì¦ (C2 ì§€ì›) |
| **ì‹¤í—˜ ë°©ë²•** | Full fine-tuningìœ¼ë¡œ Î”W*=W_t*-W_base ê³„ì‚°, SVD ìˆ˜í–‰, effective rank ê³„ì‚° |
| **ì‹¤í—˜ ë‚´ìš©, ë…¼ë¦¬** | Pre-trained backboneì´ task-agnostic í‘œí˜„ì„ ì¶”ì¶œí•˜ë¯€ë¡œ, NFì˜ ì—­í• ì€ êµ¬ì¡°ì  ë³€í™˜ì— êµ­í•œ. ë”°ë¼ì„œ task-specific adaptationì€ ì €ì°¨ì› ë¶€ë¶„ê³µê°„ì— ìœ„ì¹˜ |
| **ê²°ê³¼ í˜•íƒœ** | **Table 9**: ë ˆì´ì–´ë³„ effective rank<br>**Figure 4**: SVD spectrum plot |

**ì‹¤í—˜ ê²°ê³¼** (logs/5_Analysis/SVD/leather_to_grid_v3):

| Layer | Shape | Effective Rank (95%) | Energy at r=64 | Energy at r=128 |
|-------|-------|----------------------|----------------|-----------------|
| subnet0_layer1 | 1024Ã—512 | 397 | 29.9% | 51.2% |
| subnet0_layer2 | 1024Ã—1024 | 616 | 24.8% | 40.7% |
| subnet1_layer1 | 1024Ã—512 | 397 | 30.3% | 51.4% |
| subnet1_layer2 | 1024Ã—1024 | 615 | 25.5% | 41.4% |
| **Mean** | - | **504.3 Â± 108.3** | **28.5%** | **45.7%** |

**í•µì‹¬ ê´€ì°°**:
- **Effective rankê°€ ì˜ˆìƒë³´ë‹¤ ë†’ìŒ** (95% ì—ë„ˆì§€ ê¸°ì¤€ ~504)
- Rank 64ë¡œëŠ” ì—ë„ˆì§€ì˜ ~28.5%ë§Œ ìº¡ì²˜ â†’ **LoRA rank 64ê°€ ì¶©ë¶„í•œ ì´ìœ ëŠ” full spectrum ê·¼ì‚¬ê°€ ì•„ë‹˜**
- **í•´ì„ ìˆ˜ì •**: Low-rankê°€ íš¨ê³¼ì ì¸ ì´ìœ ëŠ” "task adaptationì´ ì €ì°¨ì›"ì´ê¸° ë•Œë¬¸ì´ ì•„ë‹ˆë¼, **LoRAê°€ ê°€ì¥ ì¤‘ìš”í•œ singular value ë°©í–¥ë§Œ ì„ íƒì ìœ¼ë¡œ í•™ìŠµ**í•˜ê¸° ë•Œë¬¸

**Figure**: `logs/5_Analysis/SVD/leather_to_grid_v3/svd_spectrum_(leather_->_grid).png`

**ìƒíƒœ**: âœ… **ì™„ë£Œ** (í•´ì„ ìˆ˜ì • í•„ìš” - C2 claim ì¬ê³ )

### 4.4.4 Gradient Redistribution by Tail-Aware Loss

| í•­ëª© | ë‚´ìš© |
|------|------|
| **ì‹¤í—˜ í•­ëª©** | TALì˜ gradient ì¬ë¶„ë°° íš¨ê³¼ ë¶„ì„ |
| **ì‹¤í—˜ ëª©ì ** | TALì´ "ì™œ" íš¨ê³¼ì ì¸ì§€ì— ëŒ€í•œ ë©”ì»¤ë‹ˆì¦˜ì  ì„¤ëª… ì œê³µ |
| **ì‹¤í—˜ ë°©ë²•** | Mean-only loss vs TALì—ì„œ gradient magnitude ë¶„í¬ ë¹„êµ, tail/bulk region gradient ratio ê³„ì‚° |
| **ì‹¤í—˜ ë‚´ìš©, ë…¼ë¦¬** | TALì˜ í•µì‹¬ ê¸°ì—¬: gradientë¥¼ tail(ì–´ë ¤ìš´ íŒ¨ì¹˜)ë¡œ ì¬ë¶„ë°°í•˜ì—¬ ê²°ì • ê²½ê³„ ì •ë°€í™”. Frozen baseì—ì„œ LoRAì˜ ì œí•œëœ ìš©ëŸ‰ì´ bulkê°€ ì•„ë‹Œ ê²½ê³„ì— ì§‘ì¤‘ë˜ì–´ì•¼ í•¨ |
| **ê²°ê³¼ í˜•íƒœ** | **Table 10**: Gradient ratio (tail vs non-tail)<br>**Figure 5**: Gradient distribution heatmap |

**ê¸°ì¡´ ê²°ê³¼**:

| Metric | Mean-Only Loss | Tail-Aware Loss |
|--------|----------------|-----------------|
| Gradient at tail | 0.022 | 0.840 |
| Gradient at non-tail | 0.019 | 0.017 |
| **Ratio** | **1.18Ã—** | **50.0Ã—** |

**í•µì‹¬ ë°œê²¬**: TALì´ tail ìœ„ì¹˜ì˜ gradientë¥¼ **42Ã—** ì¦í­

### 4.4.5 DIA Transformation Analysis

| í•­ëª© | ë‚´ìš© |
|------|------|
| **ì‹¤í—˜ í•­ëª©** | DIAê°€ í•™ìŠµí•˜ëŠ” ë³€í™˜ì˜ íŠ¹ì„± ë¶„ì„ |
| **ì‹¤í—˜ ëª©ì ** | DIAê°€ ì„ í˜• LoRAì™€ ì§ˆì ìœ¼ë¡œ ë‹¤ë¥¸ ë¹„ì„ í˜• ê¸°ì—¬ë¥¼ í•˜ëŠ”ì§€ ê²€ì¦ |
| **ì‹¤í—˜ ë°©ë²•** | DIA ì „í›„ latent distribution í†µê³„ ë¹„êµ, ë³€í™˜ì„ scale/shift/nonlinear ì„±ë¶„ìœ¼ë¡œ ë¶„í•´ |
| **ì‹¤í—˜ ë‚´ìš©, ë…¼ë¦¬** | DIAëŠ” "LoRA ìœ„ì˜ ì„ í˜• ë³´ì •"ì´ ì•„ë‹ˆë¼ ë¹„ì„ í˜• manifold ì ì‘ ìˆ˜í–‰. íŠ¹íˆ ê³ ì°¨ ëª¨ë©˜íŠ¸(ì²¨ë„, ì™œë„) ë³´ì •ì´ í•µì‹¬ |
| **ê²°ê³¼ í˜•íƒœ** | **Table 11**: DIA ì „í›„ latent statistics<br>**Figure 6**: t-SNE of latent space (before/after DIA) |

**ì˜ˆìƒ ê²°ê³¼**:

| Component | Contribution | Interpretation |
|-----------|--------------|----------------|
| Scale S_t | 38% | ë¶„ì‚° ì •ê·œí™” |
| Shift Î¼_t | 31% | í‰ê·  ì¤‘ì‹¬í™” |
| Nonlinear r_t | **31%** | ê³ ì°¨ ëª¨ë©˜íŠ¸ ë³´ì • |

### 4.4.6 Routing Mechanism Analysis

| í•­ëª© | ë‚´ìš© |
|------|------|
| **ì‹¤í—˜ í•­ëª©** | Prototype-based routing ë©”ì»¤ë‹ˆì¦˜ ë¶„ì„ |
| **ì‹¤í—˜ ëª©ì ** | 100% routing accuracyì˜ ì›ì¸ ë¶„ì„ ë° ì ì¬ì  failure case íŒŒì•… |
| **ì‹¤í—˜ ë°©ë²•** | Mahalanobis distance ë¶„í¬ ë¶„ì„, confidence margin ê³„ì‚°, ì–´ë ¤ìš´ ì¼€ì´ìŠ¤ ë¶„ì„ |
| **ì‹¤í—˜ ë‚´ìš©, ë…¼ë¦¬** | 100% accuracyëŠ” feature spaceì—ì„œ classë“¤ì´ ì¶©ë¶„íˆ ë¶„ë¦¬ë˜ì–´ ìˆê¸° ë•Œë¬¸. Margin ë¶„ì„ìœ¼ë¡œ ì´ë¥¼ ì •ëŸ‰í™” |
| **ê²°ê³¼ í˜•íƒœ** | **Figure 7**: Confidence margin ë¶„í¬<br>**Table 12**: ìœ ì‚¬í•œ class pairì˜ íŠ¹ì„± |

**ê¸°ì¡´ ê²°ê³¼**:

| Task Pair | Feature Cosine Similarity | Routing Confusion Risk |
|-----------|---------------------------|------------------------|
| carpet-grid | 0.72 | 8.3% (potential) |
| screw-metal_nut | 0.68 | 5.1% (potential) |
| Average (others) | <0.45 | <1% |

### 4.4.7 Computational Cost

| í•­ëª© | ë‚´ìš© |
|------|------|
| **ì‹¤í—˜ í•­ëª©** | Per-task parameter overhead ë° training/inference ì‹œê°„ ë¶„ì„ |
| **ì‹¤í—˜ ëª©ì ** | MoLE-Flowì˜ computational efficiency ì •ëŸ‰í™” |
| **ì‹¤í—˜ ë°©ë²•** | ì»´í¬ë„ŒíŠ¸ë³„ íŒŒë¼ë¯¸í„° ìˆ˜, GPU memory, training time, inference latency ì¸¡ì • |
| **ì‹¤í—˜ ë‚´ìš©, ë…¼ë¦¬** | Parameter isolationì˜ ë¹„ìš© íˆ¬ëª…í•˜ê²Œ ê³µê°œ. "Zero forgettingì˜ ëŒ€ê°€ê°€ ì–¼ë§ˆì¸ê°€?" |
| **ê²°ê³¼ í˜•íƒœ** | **Table 13**: Componentë³„ íŒŒë¼ë¯¸í„° breakdown<br>**Table 14**: Training/inference cost ë¹„êµ |

**ê¸°ì¡´ ê²°ê³¼**:

| Component | LoRA rank=64 | % of NF base |
|-----------|--------------|--------------|
| LoRA (A+B) | 1.92M | 21.6% |
| TaskBias | 14K | 0.2% |
| WhiteningAdapter | 1.5K | <0.1% |
| DIA (2 blocks) | 1.77M | 19.9% |
| **Total/Task** | **3.71M** | **41.7%** |

| Metric | MoLE-Flow | ReplayCAD | CADIC |
|--------|-----------|-----------|-------|
| GPU Memory | **2.6GB** | 6.8GB | 8.5GB |
| Train Time (Task 1+) | **0.7 min** | ~2 min | ~2.5 min |
| Inference | **8.9ms** | 52ms | 68ms |

---

## Summary: Experiment-Claim Mapping

| Section | Experiments | Supporting Claims |
|---------|-------------|-------------------|
| 4.2 Main Results | SOTA comparison, Cross-dataset, Zero forgetting | C3 (Zero Forgetting), C5 (Routing) |
| 4.3.1-2 Component Ablation | Ablation, Interaction effect | C4 (Structural Necessity) |
| 4.3.3 Rank Sensitivity | LoRA rank ablation | C2 (Low-Rank) |
| 4.3.5 Task 0 Sensitivity | Task 0 selection analysis | Robustness |
| 4.3.6 Long-Sequence | 30+ tasks scalability | C3, C5 Scalability |
| 4.3.7 Backbone Sensitivity | Multi-backbone validation | C2 Generality |
| 4.4.1 Architecture Comparison | NF vs VAE/AE | C1 (AFP) |
| 4.4.2 Coupling vs Feature-level | Adaptation level comparison | C1 (AFP) |
| 4.4.3 SVD Analysis | Weight update analysis | C2 (Low-Rank) |
| 4.4.6 Routing Analysis | Routing mechanism | C5 (Perfect Routing) |

---

## Critical TODO

| Priority | Experiment | Status | Claim |
|----------|------------|--------|-------|
| âœ… P0 | 4.4.1 Architecture Comparison (NF vs VAE/AE) | **ì™„ë£Œ** | C1 |
| âœ… P0 | 4.3.2 Interaction Effect (Factorial Analysis) | **ì™„ë£Œ** | C4 |
| ğŸ”´ P0 | 4.4.2 Coupling vs Feature-level Adaptation | **TODO** | C1 |
| âœ… P1 | 4.2.3 BWT Verification | **ì™„ë£Œ** | C3 |
| âœ… P1 | 4.3.1 Component Ablation | **ì™„ë£Œ** | C4 |
| âœ… P1 | 4.3.3 LoRA Rank Sensitivity | **ì™„ë£Œ** | C2 |
| âœ… P1 | 4.3.4 Architecture Depth | **ì™„ë£Œ** | - |
| âœ… P1 | 4.4.3 SVD Analysis | **ì™„ë£Œ** (í•´ì„ ìˆ˜ì • í•„ìš”) | C2 |
| âœ… P1 | 4.2.2 ViSA Routing Accuracy | **ì™„ë£Œ** (99.89%) | C5 |
| ğŸŸ¡ P1 | 4.4.5 DIA Transformation Analysis | TODO | C4 |
| ğŸŸ¡ P1 | 4.4.6 Routing Confidence Analysis | TODO | C5 |
| ğŸŸ¡ P1 | 4.3.5 Task 0 Sensitivity | TODO | Robustness |
| ğŸŸ¢ P2 | 4.3.6 Long-Sequence Scalability (30+ tasks) | TODO | Scalability |
| ğŸŸ¢ P2 | 4.3.7 Backbone Sensitivity | TODO | Generality |

---

*Document Version: 4.0 (Comprehensive update with all available experiment results)*
*Last Updated: 2026-01-21*

---

## Appendix: Figure References

| Figure | Path | Description |
|--------|------|-------------|
| Architecture Comparison | `Paper_works/figures/architecture_comparison.png` | NF vs VAE/AE/TS ë¹„êµ |
| SVD Spectrum | `Paper_works/figures/svd_spectrum.png` | Weight update SVD ë¶„ì„ |
| Effective Rank | `Paper_works/figures/effective_rank_histogram.png` | ë ˆì´ì–´ë³„ effective rank ë¶„í¬ |
| Energy at Ranks | `Paper_works/figures/energy_at_ranks.png` | Rankë³„ ì—ë„ˆì§€ ìº¡ì²˜ëŸ‰ |
| Gradient Concentration | `Paper_works/figures/fig1_gradient_concentration.png` | TAL gradient ì¬ë¶„ë°° íš¨ê³¼ |
| Ablation Heatmap | `Paper_works/figures/fig5_ablation_heatmap.png` | ì»´í¬ë„ŒíŠ¸ ablation ì‹œê°í™” |
