"""
Evaluation Functions for MoLE-Flow Continual Anomaly Detection.

Provides comprehensive evaluation metrics including:
- Image-level AUROC and Average Precision (AP)
- Pixel-level AUROC and Average Precision (AP)
- Routing accuracy and distribution analysis
"""

import numpy as np
import torch
import torch.nn.functional as F
from torch.utils.data import DataLoader
from typing import Dict, List, Optional
from scipy.ndimage import gaussian_filter
from sklearn.metrics import roc_auc_score, average_precision_score

from moleflow.utils.logger import TrainingLogger


def evaluate_class(trainer,
                   class_name: str,
                   args,
                   dataset_class,
                   task_id: Optional[int] = None,
                   ground_truth_task_id: Optional[int] = None,
                   target_size: int = 224) -> Dict:
    """
    Evaluate a single class.

    Args:
        trainer: MoLEContinualTrainer
        class_name: Class name to evaluate
        args: Configuration
        dataset_class: Dataset class to use (e.g., MVTEC)
        task_id: If None, use router for task selection
        ground_truth_task_id: Ground truth task ID for routing accuracy calculation
        target_size: Target size for pixel-level evaluation

    Returns:
        Dict containing:
            - img_auc: Image-level AUROC
            - pixel_auc: Pixel-level AUROC
            - img_ap: Image-level Average Precision
            - pixel_ap: Pixel-level Average Precision
            - routing_accuracy: Accuracy of task routing (if using router)
            - routing_distribution: Distribution of predicted tasks
            - n_samples: Number of test samples
    """
    device = trainer.device

    # Create test dataset
    test_dataset = dataset_class(args.data_path, class_name=class_name, train=False,
                                 img_size=args.img_size, crp_size=args.img_size, msk_size=target_size)

    test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False,
                            drop_last=False, pin_memory=True, num_workers=4)

    gt_label_list = []
    gt_mask_list = []
    anomaly_scores_list = []
    image_scores_list = []
    predicted_tasks_list = []

    print(f"  Evaluating {class_name}...")

    for idx, (image, label, mask, _, _) in enumerate(test_loader):
        gt_label_list.extend(label.cpu().numpy())
        gt_mask_list.extend(mask.cpu().numpy())

        # Inference
        anomaly_scores, image_scores, predicted_tasks = trainer.inference(
            image, task_id=task_id
        )

        # Resize for pixel-level evaluation
        anomaly_scores_resized = F.interpolate(
            anomaly_scores.unsqueeze(1),
            size=(target_size, target_size),
            mode='bilinear',
            align_corners=True
        ).squeeze(1)

        anomaly_scores_list.append(anomaly_scores_resized.cpu())
        image_scores_list.append(image_scores.cpu())
        predicted_tasks_list.extend(predicted_tasks.cpu().numpy())

    # Concatenate results
    anomaly_scores_all = torch.cat(anomaly_scores_list, dim=0).numpy()
    image_scores_all = torch.cat(image_scores_list, dim=0).numpy()

    gt_label = np.asarray(gt_label_list, dtype=bool)
    gt_mask = np.squeeze(np.asarray(gt_mask_list, dtype=bool), axis=1)

    # Apply Gaussian smoothing
    for i in range(anomaly_scores_all.shape[0]):
        anomaly_scores_all[i] = gaussian_filter(anomaly_scores_all[i], sigma=4)

    # Sanitize scores
    image_scores_all = np.nan_to_num(image_scores_all, nan=0.0, posinf=1.0, neginf=0.0)
    anomaly_scores_all = np.nan_to_num(anomaly_scores_all, nan=0.0, posinf=1.0, neginf=0.0)

    # Compute AUROC
    img_auc = roc_auc_score(gt_label, image_scores_all)
    pixel_auc = roc_auc_score(gt_mask.flatten(), anomaly_scores_all.flatten())

    # Compute Average Precision (AP)
    img_ap = average_precision_score(gt_label, image_scores_all)
    pixel_ap = average_precision_score(gt_mask.flatten(), anomaly_scores_all.flatten())

    # Compute routing statistics
    predicted_tasks_arr = np.array(predicted_tasks_list)
    unique_tasks, counts = np.unique(predicted_tasks_arr, return_counts=True)
    routing_distribution = dict(zip(unique_tasks.tolist(), counts.tolist()))

    # Compute routing accuracy if ground truth is available
    routing_accuracy = None
    if ground_truth_task_id is not None and task_id is None:
        correct_routing = (predicted_tasks_arr == ground_truth_task_id).sum()
        routing_accuracy = correct_routing / len(predicted_tasks_arr)

    # Logging
    if task_id is None:
        print(f"    Routing: {routing_distribution}")
        if routing_accuracy is not None:
            print(f"    Routing Accuracy: {routing_accuracy:.2%} ({int(correct_routing)}/{len(predicted_tasks_arr)})")

    return {
        'img_auc': img_auc,
        'pixel_auc': pixel_auc,
        'img_ap': img_ap,
        'pixel_ap': pixel_ap,
        'routing_accuracy': routing_accuracy,
        'routing_distribution': routing_distribution,
        'n_samples': len(predicted_tasks_arr)
    }


def evaluate_all_tasks(trainer,
                       args,
                       dataset_class,
                       use_router: bool = True,
                       target_size: int = 224,
                       logger: TrainingLogger = None) -> Dict:
    """
    Evaluate all learned tasks.

    Args:
        trainer: MoLEContinualTrainer
        args: Configuration
        dataset_class: Dataset class to use (e.g., MVTEC)
        use_router: If True, use router for task selection
        target_size: Target size for pixel-level evaluation
        logger: Optional TrainingLogger for logging

    Returns:
        results: Dict with class-wise and average metrics including routing info
    """
    results = {
        'classes': [],
        'task_ids': [],
        'img_aucs': [],
        'pixel_aucs': [],
        'img_aps': [],
        'pixel_aps': [],
        'routing_accuracies': [],
        'routing_distributions': [],
        'n_samples': [],
        # Class-wise dictionary for easy access
        'class_img_aucs': {},
        'class_pixel_aucs': {},
        'class_img_aps': {},
        'class_pixel_aps': {},
        'class_routing_accuracies': {},
    }

    eval_header = "\n" + "="*70 + "\n" + "üìä Evaluating All Tasks" + (" (with Router)" if use_router else " (with Oracle)") + "\n" + "="*70
    print(eval_header)
    if logger:
        logger.info(eval_header)

    # Safety check: ensure trainer has task classes
    if not hasattr(trainer, 'task_classes') or not trainer.task_classes:
        print("Warning: No tasks have been trained yet. Returning empty results.")
        results['mean_img_auc'] = 0.0
        results['mean_pixel_auc'] = 0.0
        results['mean_img_ap'] = 0.0
        results['mean_pixel_ap'] = 0.0
        results['task_avg_img_aucs'] = {}
        results['task_avg_pixel_aucs'] = {}
        results['task_avg_img_aps'] = {}
        results['task_avg_pixel_aps'] = {}
        return results

    for task_id, task_classes in trainer.task_classes.items():
        task_header = f"\nüìÅ Task {task_id}: {task_classes}"
        print(task_header)
        if logger:
            logger.info(task_header)

        for class_name in task_classes:
            # Use router or ground truth task
            eval_task_id = None if use_router else task_id

            class_results = evaluate_class(
                trainer, class_name, args, dataset_class,
                task_id=eval_task_id,
                ground_truth_task_id=task_id,
                target_size=target_size
            )

            img_auc = class_results['img_auc']
            pixel_auc = class_results['pixel_auc']
            img_ap = class_results['img_ap']
            pixel_ap = class_results['pixel_ap']
            routing_acc = class_results['routing_accuracy']

            results['classes'].append(class_name)
            results['task_ids'].append(task_id)
            results['img_aucs'].append(img_auc)
            results['pixel_aucs'].append(pixel_auc)
            results['img_aps'].append(img_ap)
            results['pixel_aps'].append(pixel_ap)
            results['routing_accuracies'].append(routing_acc)
            results['routing_distributions'].append(class_results['routing_distribution'])
            results['n_samples'].append(class_results['n_samples'])

            # Store in class-wise dictionary
            results['class_img_aucs'][class_name] = img_auc
            results['class_pixel_aucs'][class_name] = pixel_auc
            results['class_img_aps'][class_name] = img_ap
            results['class_pixel_aps'][class_name] = pixel_ap
            results['class_routing_accuracies'][class_name] = routing_acc

            routing_str = f", Routing Acc={routing_acc:.2%}" if routing_acc is not None else ""
            class_result = f"    {class_name}: Image AUC={img_auc:.4f}, Pixel AUC={pixel_auc:.4f}, " \
                          f"Image AP={img_ap:.4f}, Pixel AP={pixel_ap:.4f}{routing_str}"
            print(class_result)
            if logger:
                logger.info(class_result)

    # Compute averages
    results['mean_img_auc'] = np.mean(results['img_aucs'])
    results['mean_pixel_auc'] = np.mean(results['pixel_aucs'])
    results['mean_img_ap'] = np.mean(results['img_aps'])
    results['mean_pixel_ap'] = np.mean(results['pixel_aps'])

    # Compute overall routing accuracy (weighted by n_samples)
    if use_router:
        total_correct = 0
        total_samples = 0
        for i, routing_acc in enumerate(results['routing_accuracies']):
            if routing_acc is not None:
                n = results['n_samples'][i]
                total_correct += routing_acc * n
                total_samples += n
        results['mean_routing_accuracy'] = total_correct / total_samples if total_samples > 0 else None
    else:
        results['mean_routing_accuracy'] = None

    # Task-wise averages
    results['task_avg_img_aucs'] = {}
    results['task_avg_pixel_aucs'] = {}
    results['task_avg_img_aps'] = {}
    results['task_avg_pixel_aps'] = {}
    results['task_avg_routing_accuracies'] = {}

    for task_id in trainer.task_classes.keys():
        task_indices = [i for i, t in enumerate(results['task_ids']) if t == task_id]
        results['task_avg_img_aucs'][task_id] = np.mean([results['img_aucs'][i] for i in task_indices])
        results['task_avg_pixel_aucs'][task_id] = np.mean([results['pixel_aucs'][i] for i in task_indices])
        results['task_avg_img_aps'][task_id] = np.mean([results['img_aps'][i] for i in task_indices])
        results['task_avg_pixel_aps'][task_id] = np.mean([results['pixel_aps'][i] for i in task_indices])

        task_routing_accs = [results['routing_accuracies'][i] for i in task_indices if results['routing_accuracies'][i] is not None]
        if task_routing_accs:
            results['task_avg_routing_accuracies'][task_id] = np.mean(task_routing_accs)

    # Print summary (also log to file if logger is available)
    summary_lines = []
    summary_lines.append("\n" + "="*70)
    summary_lines.append("üìà Evaluation Summary")
    summary_lines.append("="*70)

    for i, class_name in enumerate(results['classes']):
        routing_str = f", Routing={results['routing_accuracies'][i]:.2%}" if results['routing_accuracies'][i] is not None else ""
        line = f"  {class_name:15s} (Task {results['task_ids'][i]}): " \
              f"I-AUC={results['img_aucs'][i]:.4f}, P-AUC={results['pixel_aucs'][i]:.4f}, " \
              f"I-AP={results['img_aps'][i]:.4f}, P-AP={results['pixel_aps'][i]:.4f}{routing_str}"
        summary_lines.append(line)
        print(line)

    summary_lines.append("-"*70)
    print("-"*70)

    for task_id in trainer.task_classes.keys():
        routing_str = ""
        if task_id in results['task_avg_routing_accuracies']:
            routing_str = f", Routing Acc={results['task_avg_routing_accuracies'][task_id]:.2%}"
        line = f"  Task {task_id} Average: " \
              f"I-AUC={results['task_avg_img_aucs'][task_id]:.4f}, " \
              f"P-AUC={results['task_avg_pixel_aucs'][task_id]:.4f}, " \
              f"I-AP={results['task_avg_img_aps'][task_id]:.4f}, " \
              f"P-AP={results['task_avg_pixel_aps'][task_id]:.4f}{routing_str}"
        summary_lines.append(line)
        print(line)

    summary_lines.append("-"*70)
    print("-"*70)
    routing_str = f", Routing Acc={results['mean_routing_accuracy']:.2%}" if results['mean_routing_accuracy'] is not None else ""
    line = f"  Overall Average: " \
          f"I-AUC={results['mean_img_auc']:.4f}, P-AUC={results['mean_pixel_auc']:.4f}, " \
          f"I-AP={results['mean_img_ap']:.4f}, P-AP={results['mean_pixel_ap']:.4f}{routing_str}"
    summary_lines.append(line)
    print(line)
    summary_lines.append("="*70)
    print("="*70)

    # Log to file if logger is available
    if logger:
        for line in summary_lines:
            logger.info(line)
        # Save evaluation metrics to logger
        logger.save_evaluation_metrics(results)

    return results


def evaluate_routing_performance(trainer,
                                  args,
                                  dataset_class,
                                  target_size: int = 224) -> Dict:
    """
    Detailed evaluation of routing performance.

    Computes:
    1. Overall routing accuracy
    2. Per-task routing accuracy
    3. Confusion matrix of routing
    4. Per-class routing distribution

    Args:
        trainer: MoLEContinualTrainer
        args: Configuration
        dataset_class: Dataset class to use (e.g., MVTEC)
        target_size: Target size for evaluation

    Returns:
        Dict with detailed routing metrics
    """
    device = trainer.device

    print("\n" + "="*70)
    print("üéØ Detailed Routing Performance Evaluation")
    print("="*70)

    # Collect routing results for all classes
    routing_results = {
        'class_name': [],
        'ground_truth_task': [],
        'predicted_tasks': [],
        'correct_count': [],
        'total_count': [],
        'accuracy': []
    }

    # For confusion matrix
    num_tasks = len(trainer.task_classes)
    confusion_matrix = np.zeros((num_tasks, num_tasks), dtype=int)

    for task_id, task_classes in trainer.task_classes.items():
        print(f"\nüìÅ Task {task_id}: {task_classes}")

        for class_name in task_classes:
            # Create test dataset
            test_dataset = dataset_class(args.data_path, class_name=class_name, train=False,
                                         img_size=args.img_size, crp_size=args.img_size, msk_size=target_size)
            test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False,
                                    drop_last=False, pin_memory=True, num_workers=4)

            predicted_tasks_list = []

            trainer.nf_model.eval()
            trainer.vit_extractor.eval()

            with torch.no_grad():
                for images, _, _, _, _ in test_loader:
                    images = images.to(device)

                    # Get routing predictions only
                    image_features = trainer.vit_extractor.get_image_level_features(images)
                    predicted_tasks = trainer.router.route(image_features)
                    predicted_tasks_list.extend(predicted_tasks.cpu().numpy())

            predicted_tasks_arr = np.array(predicted_tasks_list)

            # Compute accuracy
            correct = (predicted_tasks_arr == task_id).sum()
            total = len(predicted_tasks_arr)
            accuracy = correct / total

            routing_results['class_name'].append(class_name)
            routing_results['ground_truth_task'].append(task_id)
            routing_results['predicted_tasks'].append(predicted_tasks_arr)
            routing_results['correct_count'].append(correct)
            routing_results['total_count'].append(total)
            routing_results['accuracy'].append(accuracy)

            # Update confusion matrix
            for pred_task in predicted_tasks_arr:
                if 0 <= pred_task < num_tasks:
                    confusion_matrix[task_id, pred_task] += 1

            # Print per-class routing distribution
            unique_tasks, counts = np.unique(predicted_tasks_arr, return_counts=True)
            dist_str = ", ".join([f"T{t}:{c}" for t, c in zip(unique_tasks, counts)])
            print(f"    {class_name}: Acc={accuracy:.2%} ({correct}/{total}) | Dist: [{dist_str}]")

    # Compute overall statistics
    total_correct = sum(routing_results['correct_count'])
    total_samples = sum(routing_results['total_count'])
    overall_accuracy = total_correct / total_samples

    # Compute per-task statistics
    task_accuracies = {}
    for task_id in trainer.task_classes.keys():
        task_indices = [i for i, t in enumerate(routing_results['ground_truth_task']) if t == task_id]
        task_correct = sum([routing_results['correct_count'][i] for i in task_indices])
        task_total = sum([routing_results['total_count'][i] for i in task_indices])
        task_accuracies[task_id] = task_correct / task_total if task_total > 0 else 0.0

    routing_results['overall_accuracy'] = overall_accuracy
    routing_results['task_accuracies'] = task_accuracies
    routing_results['confusion_matrix'] = confusion_matrix

    # Print summary
    print("\n" + "="*70)
    print("üìä Routing Summary")
    print("="*70)

    print(f"\n  Overall Routing Accuracy: {overall_accuracy:.2%} ({total_correct}/{total_samples})")

    print("\n  Per-Task Routing Accuracy:")
    for task_id, acc in task_accuracies.items():
        classes = trainer.task_classes[task_id]
        print(f"    Task {task_id} ({', '.join(classes)}): {acc:.2%}")

    print("\n  Confusion Matrix (rows=GT, cols=Predicted):")
    task_labels = [f"T{i}" for i in range(num_tasks)]
    print("       " + "  ".join([f"{l:>6}" for l in task_labels]))
    for i, row in enumerate(confusion_matrix):
        row_str = "  ".join([f"{v:>6}" for v in row])
        print(f"    {task_labels[i]}:  {row_str}")

    print("="*70)

    return routing_results


def compare_router_vs_oracle(trainer, args, dataset_class,
                              continual_tasks: List[List[str]],
                              target_size: int = 224,
                              logger: TrainingLogger = None) -> Dict:
    """
    Compare performance between Router (automatic task selection) and Oracle (ground truth task).

    This helps identify whether issues are:
    - Routing problem (oracle >> router)
    - NF/LoRA adaptation problem (oracle approx router, both low)

    Args:
        trainer: MoLEContinualTrainer
        args: Configuration
        dataset_class: Dataset class to use (e.g., MVTEC)
        continual_tasks: List of task class lists
        target_size: Target size for evaluation
        logger: Optional TrainingLogger

    Returns:
        Dict with comparison results
    """
    print("\n" + "="*70)
    print("üîç Routing Performance Analysis")
    print("="*70)
    print("Comparing Oracle (ground truth task_id) vs Router (predicted task_id)")
    print("This helps identify whether the issue is:")
    print("  Routing problem (oracle >> router)")
    print("  NF/LoRA adaptation problem (oracle approx router, both low)")
    print("="*70)

    # Evaluate with Router
    print("\nüìç Evaluating with Router...")
    try:
        router_results = evaluate_all_tasks(
            trainer, args, dataset_class, use_router=True, target_size=target_size, logger=logger
        )
    except Exception as e:
        print(f"Error during router evaluation: {e}")
        router_results = None

    # Evaluate routing performance in detail
    print("\nüìç Evaluating Routing Accuracy...")
    try:
        routing_metrics = evaluate_routing_performance(trainer, args, dataset_class, target_size=target_size)
    except Exception as e:
        print(f"Error during routing evaluation: {e}")
        routing_metrics = None

    # Evaluate with Oracle
    print("\nüìç Evaluating with Oracle (ground truth task_id)...")
    try:
        oracle_results = evaluate_all_tasks(
            trainer, args, dataset_class, use_router=False, target_size=target_size, logger=logger
        )
    except Exception as e:
        print(f"Error during oracle evaluation: {e}")
        oracle_results = None

    # Compare results
    comparison = {
        'router_results': router_results,
        'oracle_results': oracle_results,
        'routing_metrics': routing_metrics,
    }

    if router_results is not None and oracle_results is not None:
        print("\n" + "="*70)
        print("üìä Performance Comparison: Router vs Oracle")
        print("="*70)

        print(f"{'Class':<15} {'Router AUC':<12} {'Oracle AUC':<12} {'AUC Gap':<10} {'Route Acc':<10} {'Diagnosis'}")
        print("-"*70)

        routing_issues = []
        nf_issues = []
        good_performance = []

        for task_id, task_classes in enumerate(continual_tasks):
            for class_name in task_classes:
                router_auc = router_results.get('class_img_aucs', {}).get(class_name, 0.0)
                oracle_auc = oracle_results.get('class_img_aucs', {}).get(class_name, 0.0)
                routing_acc = router_results.get('class_routing_accuracies', {}).get(class_name, None)
                gap = oracle_auc - router_auc

                routing_acc_str = f"{routing_acc:.1%}" if routing_acc is not None else "N/A"

                # Diagnosis
                if oracle_auc > 0.85:
                    if gap > 0.10:
                        diagnosis = "üî¥ Routing Issue"
                        routing_issues.append((class_name, gap, routing_acc))
                    else:
                        diagnosis = "‚úÖ Good"
                        good_performance.append(class_name)
                else:
                    if gap > 0.10:
                        diagnosis = "üü° Both Issues"
                        routing_issues.append((class_name, gap, routing_acc))
                        nf_issues.append((class_name, oracle_auc))
                    else:
                        diagnosis = "üî¥ NF/LoRA Issue"
                        nf_issues.append((class_name, oracle_auc))

                print(f"{class_name:<15} {router_auc:<12.4f} {oracle_auc:<12.4f} "
                      f"{gap:>+9.4f} {routing_acc_str:<10} {diagnosis}")

        print("="*70)

        comparison['routing_issues'] = routing_issues
        comparison['nf_issues'] = nf_issues
        comparison['good_performance'] = good_performance

        # Summary statistics
        print("\n" + "="*70)
        print("Summary Statistics")
        print("="*70)

        router_mean_img = router_results.get('mean_img_auc', 0.0)
        oracle_mean_img = oracle_results.get('mean_img_auc', 0.0)
        router_mean_pixel = router_results.get('mean_pixel_auc', 0.0)
        oracle_mean_pixel = oracle_results.get('mean_pixel_auc', 0.0)

        print(f"Overall Mean Image AUC:")
        print(f"  - With Router:  {router_mean_img:.4f}")
        print(f"  - With Oracle:  {oracle_mean_img:.4f}")
        print(f"  - Gap:          {oracle_mean_img - router_mean_img:+.4f}")
        print()
        print(f"Overall Mean Pixel AUC:")
        print(f"  - With Router:  {router_mean_pixel:.4f}")
        print(f"  - With Oracle:  {oracle_mean_pixel:.4f}")
        print(f"  - Gap:          {oracle_mean_pixel - router_mean_pixel:+.4f}")

        if routing_metrics and 'overall_accuracy' in routing_metrics:
            print(f"\nOverall Routing Accuracy: {routing_metrics['overall_accuracy']:.2%}")

        print("="*70)

    return comparison
