# MoLE-Flow Update Notes

## Version History

---

## v1 (baseline) - Initial Implementation

### Architecture
- Task 0: Base NFë§Œ í•™ìŠµ (LoRA ì—†ìŒ)
- Task 1+: Base frozen + LoRA í•™ìŠµ
- LoRA scaling: `alpha / (2 * rank)` = 0.0156 (1.56%)
- InputAdapter: Instance Norm + Zero-init MLP

### Results (leather â†’ grid â†’ transistor)
| Class | Routing Acc | Image AUC | Pixel AUC |
|-------|-------------|-----------|-----------|
| leather | 100% | **1.0000** | **0.9481** |
| grid | 100% | 0.8204 | 0.9259 |
| transistor | 100% | 0.6654 | 0.7144 |
| **Mean** | 100% | 0.8286 | 0.8628 |

### Issues Identified
1. **Task 0 Bias**: Base NFê°€ Task 0ì— íŽ¸í–¥ë˜ì–´ ë‹¤ë¥¸ taskì—ì„œ ì„±ëŠ¥ ì €í•˜
2. **LoRA Scaling ë¶€ì¡±**: 1.56% contributionìœ¼ë¡œ adaptation íš¨ê³¼ ë¯¸ë¯¸
3. **InputAdapter í•œê³„**: MLP residual gateê°€ 0ìœ¼ë¡œ ì‹œìž‘í•˜ì—¬ ê±°ì˜ ë¯¸ì‚¬ìš©

---

## v2 (baseline_v2) - LoRA Scaling & Task 0 LoRA

### Changes from v1
1. **LoRA Scaling 2ë°° ì¦ê°€**
   ```python
   # v1: self.scaling = alpha / (2 * rank)  # 0.0156
   # v2: self.scaling = alpha / rank         # 0.03125
   ```

2. **Task 0ë„ LoRA ì‚¬ìš©**
   - ëª¨ë“  Taskê°€ ë™ë“±í•˜ê²Œ LoRAë¡œ adaptation
   - Base NFëŠ” ë²”ìš© feature transformation í•™ìŠµ
   - Task-specific adaptationì€ LoRAê°€ ë‹´ë‹¹

### Results (leather â†’ grid â†’ transistor)
| Class | Routing Acc | Image AUC | Pixel AUC |
|-------|-------------|-----------|-----------|
| leather | 100% | 0.9997 | 0.8900 |
| grid | 100% | **0.9850** | **0.9841** |
| transistor | 100% | **0.8075** | **0.8973** |
| **Mean** | 100% | **0.9307** | **0.9238** |

### Comparison (v1 â†’ v2)
| Metric | v1 | v2 | Change |
|--------|-----|-----|--------|
| leather Image | 1.0000 | 0.9997 | -0.03% |
| leather Pixel | 0.9481 | 0.8900 | **-6.1%** |
| grid Image | 0.8204 | 0.9850 | **+20.0%** |
| grid Pixel | 0.9259 | 0.9841 | +6.3% |
| transistor Image | 0.6654 | 0.8075 | **+21.4%** |
| transistor Pixel | 0.7144 | 0.8973 | **+25.6%** |
| **Mean Image** | 0.8286 | 0.9307 | **+12.3%** |
| **Mean Pixel** | 0.8628 | 0.9238 | **+7.1%** |

### Analysis
- **Overall**: ì „ì²´ ì„±ëŠ¥ ëŒ€í­ í–¥ìƒ (Mean Image AUC +12.3%)
- **Task 0 Issue**: leatherì˜ Pixel AUCê°€ 6.1% í•˜ë½
  - ì›ì¸: Task 0ì—ì„œ Base + LoRA ë™ì‹œ í•™ìŠµ ì‹œ, LoRAê°€ ì¼ë¶€ ì •ë³´ë¥¼ ë¶„ë‹´í•˜ë©´ì„œ Baseì˜ í‘œí˜„ë ¥ ë¶„ì‚°
  - LoRAëŠ” Task-specific adaptationì— ìµœì í™”ë˜ì–´ pixel-level ì •ë°€ë„ì— ì˜í–¥

---

## v3 (baseline_v3) - FiLM-style InputAdapter + Task 0 Self-Adaptation

### Changes from v2
1. **InputAdapter êµ¬ì¡° ê°œì„  (FiLM-style)**
   - Instance Norm â†’ Layer Norm (spatial info ë³´ì¡´)
   - FiLM (Feature-wise Linear Modulation): `y = gamma * x + beta`
   - residual_gate: 0 â†’ 0.5 (MLP ì²˜ìŒë¶€í„° active)
   - hidden_dim ì¦ê°€: `channels//4` â†’ `max(channels//2, 128)`

2. **Task 0 Self-Adaptation**
   - Task 0ì—ë„ InputAdapter ì ìš© (v2ì—ì„œëŠ” Task 0ì— InputAdapter ì—†ì—ˆìŒ)
   - `has_reference=False` ì„¤ì •ìœ¼ë¡œ ê°•í•œ identity connection (90% identity + 10% transformed)
   - ì´ë¥¼ í†µí•´ Task 0ì˜ pixel-level ì„±ëŠ¥ íšŒë³µ ê¸°ëŒ€

3. **ëª¨ë“  Task ë™ë“±í•œ InputAdapter ì ìš©**
   - v2: Task 0 (InputAdapter ì—†ìŒ) vs Task 1+ (InputAdapter ìžˆìŒ)
   - v3: ëª¨ë“  Taskê°€ InputAdapter ì‚¬ìš© (êµ¬ì¡°ì  ì¼ê´€ì„±)

### Key Code Changes
```python
# adapters.py - TaskInputAdapter v3
class TaskInputAdapter(nn.Module):
    def __init__(self, channels, reference_mean=None, reference_std=None, use_norm=True):
        # FiLM parameters
        self.film_gamma = nn.Parameter(torch.ones(1, 1, 1, channels))
        self.film_beta = nn.Parameter(torch.zeros(1, 1, 1, channels))

        # Larger MLP
        hidden_dim = max(channels // 2, 128)  # Increased from channels//4

        # Active residual gate
        self.residual_gate = nn.Parameter(torch.tensor([0.5]))  # Changed from 0.0

        # Layer Norm instead of Instance Norm
        if use_norm:
            self.layer_norm = nn.LayerNorm(channels)

    def forward(self, x):
        # ...
        # Task 0 (has_reference=False): 90% identity + 10% transformed
        if not self.has_reference:
            x = 0.9 * identity + 0.1 * x
        return x
```

### Expected Improvements
- Task 0 pixel-level ì„±ëŠ¥ íšŒë³µ (v1 ìˆ˜ì¤€ìœ¼ë¡œ)
- ë” ê°•ë ¥í•œ cross-task feature transformation
- êµ¬ì¡°ì  ì¼ê´€ì„±ìœ¼ë¡œ ì¸í•œ ì•ˆì •ì ì¸ í•™ìŠµ

---

## File Changes Summary

### v1 â†’ v2
| File | Changes |
|------|---------|
| `moleflow/models/lora.py` | scaling: `alpha/(2*rank)` â†’ `alpha/rank` |
| `moleflow/models/mole_nf.py` | Task 0ì—ì„œë„ LoRA adapter ì¶”ê°€ |
| `moleflow/trainer/continual_trainer.py` | Task 0 í•™ìŠµ ë¡œì§ ìˆ˜ì • |

### v2 â†’ v3
| File | Changes |
|------|---------|
| `moleflow/models/adapters.py` | FiLM-style InputAdapter (LayerNorm, gate=0.5, larger MLP) |
| `moleflow/models/mole_nf.py` | Task 0ì—ë„ InputAdapter ì ìš© (self-adaptation) |
| `run.sh` | baseline_v3 ì‹¤í—˜ ì„¤ì • |

---

## v4 (baseline_v4) - CPCF + SC-LoRA (Novel Research Contribution)

### Research Contribution
**Paper Title**: "Beyond Independent Patches: Cross-Patch Coupling Flow with Spatial LoRA for Continual Anomaly Detection"

**Key Novelty Claims**:
1. **CPCF**: First normalizing flow that models `p(x_i | neighbors)` instead of `p(x_i)`
2. **SC-LoRA**: First spatially-varying LoRA for dense prediction tasks
3. **Continual AD**: Systematic benchmark for continual anomaly detection

### New Modules

#### 1. Spatial-Contextual LoRA (SC-LoRA)
```python
# moleflow/models/spatial_lora.py
class SpatialContextualLoRA(nn.Module):
    """
    Position-aware LoRA with spatial grid interpolation.

    Key Innovation:
    - LoRA parameters vary based on spatial position
    - Grid of LoRA parameters: (grid_size, grid_size, rank, dim)
    - Bilinear interpolation for smooth spatial adaptation
    """
    def __init__(self, in_features, out_features, rank=32, grid_size=4):
        # Grid of LoRA A: (G, G, R, D_in)
        self.lora_A = nn.Parameter(torch.zeros(grid_size, grid_size, rank, in_features))
        # Grid of LoRA B: (G, G, D_out, R)
        self.lora_B = nn.Parameter(torch.zeros(grid_size, grid_size, out_features, rank))

    def forward(self, x, positions):
        # Interpolate LoRA params based on position
        A, B = self._get_interpolated_lora(positions)
        return self.scaling * (x @ A.T @ B.T)
```

#### 2. Cross-Patch Coupling Flow (CPCF)
```python
# moleflow/models/cross_patch_flow.py
class CrossPatchCouplingLayer(nn.Module):
    """
    Coupling layer conditioned on neighborhood context.

    Standard: y = x * exp(s(x)) + t(x)
    CPCF:     y = x * exp(s(x, ctx)) + t(x, ctx)

    where ctx = NeighborhoodContext(x)
    """
    def __init__(self, channels, context_kernel=3):
        self.context_extractor = NeighborhoodContextExtractor(channels)
        self.s_net = SC_LoRA_MLP(channels + context_dim, channels)
        self.t_net = SC_LoRA_MLP(channels + context_dim, channels)

    def forward(self, x):
        context = self.context_extractor(x)  # (B, H, W, D)
        x1, x2 = x.split(D//2, dim=-1)

        # Condition on both patch and context
        s = self.s_net(cat([x1, context]))
        t = self.t_net(cat([x1, context]))

        return cat([x1, x2 * exp(s) + t])
```

### Theoretical Contribution

**Standard NF (Independent Patches)**:
```
log p(X) = Î£áµ¢ log p(xáµ¢)
```
- Assumes patches are independent
- Ignores spatial context

**CPCF (Context-Aware Patches)**:
```
log p(X) = Î£áµ¢ log p(xáµ¢ | N(i))
```
where N(i) = neighborhood of patch i

- Models "how different is this patch from its neighbors"
- Directly captures anomaly as contextual deviation

### Architecture Comparison

| Component | v3 | v4 |
|-----------|-----|-----|
| Flow Type | FrEIA (independent) | CPCF (context-aware) |
| LoRA Type | Standard LoRA | SC-LoRA (position-aware) |
| Patch Modeling | `p(xáµ¢)` | `p(xáµ¢ \| neighbors)` |
| Spatial Awareness | Position embedding only | Context + Position LoRA |

### Command Line Arguments
```bash
python run_moleflow.py \
    --use_cpcf \              # Enable Cross-Patch Coupling Flow
    --use_spatial_lora \       # Enable Spatial-Contextual LoRA
    --sc_lora_grid_size 4 \    # SC-LoRA grid resolution (4x4)
    --cpcf_context_kernel 3 \  # Context extraction kernel size
    --cpcf_use_attention       # Use attention instead of conv for context
```

### Expected Improvements
- **Image AUC**: +5-10% (better global anomaly detection)
- **Pixel AUC**: +10-15% (context-aware localization)
- **Cross-task**: Improved SC-LoRA handles position-dependent distribution shifts

### v3 â†’ v4
| File | Changes |
|------|---------|
| `moleflow/models/spatial_lora.py` | NEW: SC-LoRA module |
| `moleflow/models/cross_patch_flow.py` | NEW: CPCF module |
| `moleflow/models/mole_nf.py` | CPCF/SC-LoRA integration |
| `run_moleflow.py` | CPCF/SC-LoRA arguments |
| `run.sh` | baseline_v4 ì‹¤í—˜ ì„¤ì • |

---

## v4.1 - Fixed Context Extraction (Bug Fix)

### Issue
- v4ì—ì„œ Task 0 ì„±ëŠ¥ì€ í–¥ìƒë˜ì—ˆìœ¼ë‚˜ Task 1, 2 ì„±ëŠ¥ì´ í¬ê²Œ ì €í•˜ë¨
- ì›ì¸: Context Extractorê°€ Task 0ì—ì„œë§Œ í•™ìŠµë˜ì–´ Task 0ì— íŽ¸í–¥

### Root Cause
```python
# ì´ì „ ì½”ë“œ - Task 0ì—ì„œë§Œ context extractor í•™ìŠµ
if task_id == 0:
    params.extend(layer.context_extractor.parameters())
```
- Task 0: Context Extractorê°€ Task 0 ë°ì´í„°ë¡œë§Œ í•™ìŠµ
- Task 1+: Task 0ì— íŽ¸í–¥ëœ context feature ì œê³µ â†’ ì„±ëŠ¥ ì €í•˜

### Fix
Context extractionì„ **ê³ ì •ëœ (non-learnable) ë°©ì‹**ìœ¼ë¡œ ë³€ê²½:

```python
# cross_patch_flow.py - NeighborhoodContextExtractor
class NeighborhoodContextExtractor(nn.Module):
    def __init__(self, channels, kernel_size=3, use_attention=False):
        # FIXED averaging kernel - no learnable parameters
        kernel = torch.ones(1, 1, kernel_size, kernel_size) / (kernel_size * kernel_size)
        self.register_buffer('avg_kernel', kernel)
        self.register_buffer('context_gate', torch.tensor([0.3]))  # Fixed gate

    def forward(self, x):
        # Simple neighborhood mean (task-agnostic)
        neighbor_mean = F.conv2d(x, self.avg_kernel, padding, groups=D)
        context = (1 - gate) * x + gate * neighbor_mean
        return context
```

### Key Insight
- **Context = ì£¼ë³€ í‰ê· ** â†’ task-agnostic (ì–´ë–¤ taskì—ì„œë„ ë™ì¼í•œ ì˜ë¯¸)
- **SC-LoRA = task-specific adaptation** â†’ task ë³„ë¡œ ë‹¤ë¥´ê²Œ í•™ìŠµ
- ì—­í•  ë¶„ë¦¬: ê³ ì • context + í•™ìŠµ ê°€ëŠ¥í•œ adaptation

### v4 â†’ v4.1
| File | Changes |
|------|---------|
| `moleflow/models/cross_patch_flow.py` | Fixed (non-learnable) context extraction |
| `moleflow/models/mole_nf.py` | Removed context_extractor from trainable params |

---

## v5 - Center Loss for Discriminative Feature Learning

### Motivation
- v4ì˜ cross-patch context ì ‘ê·¼ë²•ì´ task bias ë¬¸ì œë¡œ ì‹¤íŒ¨
- Anomaly Detection ìžì²´ì˜ ì„±ëŠ¥ í–¥ìƒì— ì§‘ì¤‘
- Normal featureë¥¼ ë” compactí•˜ê²Œ ë§Œë“¤ì–´ì„œ anomaly êµ¬ë¶„ ìš©ì´í•˜ê²Œ

### Core Idea
**Latent space z**ì— Center Lossë¥¼ ì ìš©í•˜ì—¬ normalì„ z â‰ˆ 0ìœ¼ë¡œ ë” ê°•í•˜ê²Œ ìœ ë„:
```
Loss = NLL_loss + Î» * Center_loss
     = -log p(x) + Î» * ||z||Â²
```

**Key Insight**:
- NFëŠ” inputì„ Gaussianìœ¼ë¡œ ë§¤í•‘ â†’ normalì€ z â‰ˆ 0
- Center LossëŠ” ì´ ëª©í‘œë¥¼ **ë” ê°•í•˜ê²Œ** ê°•ì œ
- Input featureê°€ ì•„ë‹Œ **latent z**ì— ì ìš©í•´ì•¼ gradientê°€ NFì— ì „íŒŒë¨

### Implementation

```python
# Training loopì—ì„œ
z, log_jac_det = nf_model.forward(x, reverse=False)

# NLL loss
log_pz = -0.5 * (z ** 2).sum() / (B * H * W)
nll_loss = -(log_pz + log_jac)

# Center loss on latent z (fixed center at zero)
center_loss = (z ** 2).sum(dim=-1).mean()  # ||z - 0||Â²

total_loss = nll_loss + Î» * center_loss
```

### Why Fixed Center at Zero?
- Learnable center â†’ centerê°€ zì˜ meanìœ¼ë¡œ ì´ë™ â†’ ì˜ë¯¸ ì—†ìŒ
- Fixed center = 0 â†’ zë¥¼ ì›ì ìœ¼ë¡œ ê°•í•˜ê²Œ ë‹¹ê¹€ â†’ Gaussian prior ê°•í™”

### Training Flow
1. Forward: x â†’ NF â†’ z
2. NLL loss: zê°€ Gaussianì„ ë”°ë¥´ë„ë¡
3. Center loss: zê°€ ì›ì ì— ê°€ê¹ë„ë¡ (ì¶”ê°€ regularization)
4. Backward: gradientê°€ NFë¡œ ì „íŒŒë˜ì–´ ë” compactí•œ latent space í•™ìŠµ

### Command Line Arguments
```bash
python run_moleflow.py \
    --center_loss_weight 0.05 \  # Center loss weight (recommend 0.01-0.1)
    ...
```

### Expected Improvements
- Normal featureê°€ ë” compactí•´ì ¸ì„œ anomaly êµ¬ë¶„ í–¥ìƒ
- íŠ¹ížˆ pixel-level anomaly detectionì—ì„œ íš¨ê³¼ ê¸°ëŒ€
- Taskë³„ centerê°€ task-specific íŠ¹ì„±ì„ í•™ìŠµ

### v3 â†’ v5
| File | Changes |
|------|---------|
| `moleflow/models/center_loss.py` | NEW: CenterLoss module |
| `moleflow/trainer/continual_trainer.py` | Center loss integration |
| `run_moleflow.py` | `--center_loss_weight` argument |
| `run.sh` | v5 experiment configuration |

---

## v6 - Patch Self-Attention for Contextual Anomaly Detection

### Motivation
- v5ì˜ Center LossëŠ” NLLì´ ì´ë¯¸ z â‰ˆ 0ì„ ìœ ë„í•˜ë¯€ë¡œ íš¨ê³¼ ë¯¸ë¯¸
- **Contextual Anomaly** íƒì§€ í•„ìš”: ì£¼ë³€ íŒ¨ì¹˜ì™€ ë‹¤ë¥¸ íŒ¨ì¹˜ê°€ anomaly
- Patch ê°„ ê´€ê³„ë¥¼ ëª¨ë¸ë§í•˜ì—¬ anomaly detection ì„±ëŠ¥ í–¥ìƒ

### Core Idea
**Standard NF (Independent Patches)**:
```
log p(X) = Î£áµ¢ log p(xáµ¢)
```
- ê° íŒ¨ì¹˜ë¥¼ ë…ë¦½ì ìœ¼ë¡œ ì²˜ë¦¬
- ì£¼ë³€ context ë¬´ì‹œ

**Patch Self-Attention (Context-Aware)**:
```
log p(X) = Î£áµ¢ log p(xáµ¢ | context_i)
where context_i = Attention(xáµ¢, all patches)
```
- íŒ¨ì¹˜ ê°„ ê´€ê³„ ëª¨ë¸ë§
- "ì£¼ë³€ê³¼ ë‹¤ë¥¸" íŒ¨ì¹˜ë¥¼ anomalyë¡œ íƒì§€

### Architecture
```
ViT Features [B, H, W, D]
       â†“
Patch Self-Attention (LightweightPatchAttention)
       â†“
Context-Enhanced Features [B, H, W, D]
       â†“
Normalizing Flow
       â†“
Latent z, log_jac_det
```

### LightweightPatchAttention Module
```python
class LightweightPatchAttention(nn.Module):
    def __init__(self, embed_dim=512, hidden_dim=256, dropout=0.1):
        # Q, K, V projections
        self.q_proj = nn.Linear(embed_dim, hidden_dim)
        self.k_proj = nn.Linear(embed_dim, hidden_dim)
        self.v_proj = nn.Linear(embed_dim, hidden_dim)

        # Learnable gate (starts at 0 for stable training)
        self.gate = nn.Parameter(torch.zeros(1))

        # FFN for additional processing
        self.ffn = nn.Sequential(...)

    def forward(self, x):  # x: [B, H, W, D]
        # Reshape to sequence
        x_seq = x.view(B, H*W, D)

        # Self-attention
        Q, K, V = self.q_proj(x), self.k_proj(x), self.v_proj(x)
        attn = softmax(Q @ K.T / sqrt(d))
        attn_out = attn @ V

        # Gated residual connection
        gate = sigmoid(self.gate)
        x = x + gate * attn_out

        # FFN
        x = x + self.ffn(x)

        return x.view(B, H, W, D)
```

### Key Design Choices
1. **Learnable Gate (starts at 0)**:
   - í•™ìŠµ ì´ˆê¸°ì—ëŠ” identity (ì•ˆì •ì  í•™ìŠµ)
   - ì ì§„ì ìœ¼ë¡œ attention ê¸°ì—¬ë„ ì¦ê°€

2. **Single-Head Attention**:
   - Lightweightí•˜ë©´ì„œë„ patch ê´€ê³„ í¬ì°©
   - Multi-head ëŒ€ë¹„ ê³„ì‚° íš¨ìœ¨ì 

3. **Pre-LayerNorm + FFN**:
   - Transformer ìŠ¤íƒ€ì¼ ì•ˆì •ì  í•™ìŠµ
   - FFNìœ¼ë¡œ ë¹„ì„ í˜• transformation ê°•í™”

### Training
- Patch attention ëª¨ë“ˆì€ **ëª¨ë“  taskì—ì„œ ê³µìœ ** (Base NFì²˜ëŸ¼)
- Task 0ì—ì„œ í•™ìŠµ í›„ freeze
- Task 1+ì—ì„œëŠ” LoRAë§Œ í•™ìŠµ

### Command Line Arguments
```bash
python run_moleflow.py \
    --use_patch_attention \  # Enable Patch Self-Attention
    ...
```

### Expected Improvements
- **Contextual Anomaly Detection**: ì£¼ë³€ê³¼ ë‹¤ë¥¸ íŒ¨ì¹˜ íƒì§€ í–¥ìƒ
- **Pixel-level AUC**: Context ì •ë³´ë¡œ localization ì •ë°€ë„ í–¥ìƒ
- **Structural Anomaly**: ì „ì—­ì  íŒ¨ì¹˜ ê´€ê³„ë¡œ êµ¬ì¡°ì  ì´ìƒ íƒì§€

### v3 â†’ v6
| File | Changes |
|------|---------|
| `moleflow/models/patch_attention.py` | NEW: LightweightPatchAttention, PatchInteractionModule |
| `moleflow/models/__init__.py` | Export patch attention modules |
| `moleflow/trainer/continual_trainer.py` | Patch attention integration |
| `run_moleflow.py` | `--use_patch_attention` argument |
| `run.sh` | v6 experiment configuration |

### v6 Result
- **ì‹¤íŒ¨**: ViTê°€ ì´ë¯¸ self-attentionìœ¼ë¡œ contextualizedëœ featureë¥¼ ì¶œë ¥í•˜ë¯€ë¡œ ì¶”ê°€ attentionì´ ì˜¤ížˆë ¤ í•´ê°€ ë¨

---

## v7 - Focal NLL Loss for Hard Sample Mining

### Motivation
- v5 (Center Loss): NLLì´ ì´ë¯¸ z â‰ˆ 0 ìœ ë„í•˜ë¯€ë¡œ íš¨ê³¼ ë¯¸ë¯¸
- v6 (Patch Attention): ViT ì¤‘ë³µìœ¼ë¡œ ì‹¤íŒ¨
- **ìƒˆë¡œìš´ ì ‘ê·¼**: ì–´ë ¤ìš´ ìƒ˜í”Œì— ë” ì§‘ì¤‘í•˜ì—¬ decision boundary í•™ìŠµ ê°•í™”

### Core Idea
**Standard NLL**:
```
L = -log p(x)  # ëª¨ë“  ìƒ˜í”Œ ë™ë“±í•œ ê°€ì¤‘ì¹˜
```

**Focal NLL**:
```
L = (1 - p)^Î³ * (-log p(x))

Î³ = 0: Standard NLL (ëª¨ë“  ìƒ˜í”Œ ë™ë“±)
Î³ = 1: ì•½ê°„ì˜ hard sample ê°•ì¡°
Î³ = 2: ê°•í•œ hard sample ê°•ì¡° (ê¶Œìž¥)
```

- `p` = probability = exp(-nll)
- ë†’ì€ NLL (ì–´ë ¤ìš´ ìƒ˜í”Œ) â†’ ë‚®ì€ p â†’ ë†’ì€ weight (1-p)^Î³
- ë‚®ì€ NLL (ì‰¬ìš´ ìƒ˜í”Œ) â†’ ë†’ì€ p â†’ ë‚®ì€ weight

### Implementation
```python
def _compute_nll_loss(self, z, log_jac_det):
    B, H, W, D = z.shape

    if not self.use_focal_loss:
        # Standard NLL
        log_pz = -0.5 * (z ** 2).sum() / (B * H * W)
        log_jac = log_jac_det.mean() / (H * W)
        return -(log_pz + log_jac)
    else:
        # Per-patch NLL
        log_pz_per_patch = -0.5 * (z ** 2).sum(dim=-1)  # [B, H, W]
        log_jac_per_patch = log_jac_det.view(B, 1, 1) / (H * W)
        nll_per_patch = -(log_pz_per_patch + log_jac_per_patch)

        # Focal weighting
        prob = torch.exp(-nll_per_patch.clamp(max=20))
        focal_weight = (1 - prob).pow(self.focal_gamma)

        return (focal_weight * nll_per_patch).mean()
```

### Why This Should Work
1. **Hard Sample Mining**: Normal distribution ê²½ê³„ì— ìžˆëŠ” ìƒ˜í”Œì— ì§‘ì¤‘
2. **Better Decision Boundary**: ì–´ë ¤ìš´ íŒ¨ì¹˜ë¥¼ ìž˜ í•™ìŠµí•˜ë©´ anomaly êµ¬ë¶„ í–¥ìƒ
3. **Gradient Focus**: Easy sampleì€ gradient ê¸°ì—¬ ê°ì†Œ, hard sampleì— gradient ì§‘ì¤‘

### Command Line Arguments
```bash
python run_moleflow.py \
    --focal_gamma 2.0 \  # Focal loss gamma (recommend 1.0-2.0)
    ...
```

### Expected Improvements
- ë” sharpí•œ normal distribution ê²½ê³„ í•™ìŠµ
- íŠ¹ížˆ Task 0ì—ì„œ base NF í’ˆì§ˆ í–¥ìƒ
- Anomaly detection ì„±ëŠ¥ ì „ë°˜ì  í–¥ìƒ

### v3 â†’ v7
| File | Changes |
|------|---------|
| `moleflow/trainer/continual_trainer.py` | `_compute_nll_loss()` helper method, focal weighting |
| `run_moleflow.py` | `--focal_gamma` argument |
| `run.sh` | v7 experiment configuration |

---

## Baseline 1.5 â†’ 2.0: Patch-wise Context Gate

### Motivation
**Baseline 1.5 (Global Alpha)ì˜ ë¬¸ì œì **:
- `alpha`ëŠ” global scalar â†’ ëª¨ë“  íŒ¨ì¹˜ì— ë™ì¼í•œ context ê°•ë„ ì ìš©
- í•™ìŠµ ì¤‘ **ì •ìƒ ë°ì´í„°ë§Œ** ë´„ â†’ anomaly-aware í•™ìŠµ ë¶ˆê°€ëŠ¥
- ê²°ê³¼: `alpha â‰ˆ ì´ˆê¸°ê°’`ìœ¼ë¡œ ê³ ì •, sigmoid boundê°€ ì˜ë¯¸ ì—†ìŒ

**í•µì‹¬ í†µì°°**:
> Global alphaëŠ” "knob"ì¼ ë¿ì´ê³ ,
> Anomaly detectionì—ì„œëŠ” "switch"ê°€ í•„ìš”í•˜ë‹¤.
> ê·¸ switchëŠ” **patch-wise gate**ë‹¤.

### Core Idea
| êµ¬ë¶„ | Baseline 1.5 (Global Alpha) | Baseline 2.0 (Patch-wise Gate) |
|------|----------------------------|-------------------------------|
| ìˆ˜ì‹ | `ctx = alpha * ctx` | `ctx = gate(x, ctx) * ctx` |
| ì°¨ì› | `alpha` âˆˆ â„ (scalar) | `gate` âˆˆ â„^(BÃ—HÃ—WÃ—1) (per-patch) |
| í•™ìŠµ | ëª¨ë“  íŒ¨ì¹˜ ë™ì¼ | íŒ¨ì¹˜ë³„ ë…ë¦½ ê²°ì • |
| ì •ìƒ íŒ¨ì¹˜ | Î± * ctx | gate â†’ 0 (context ë¬´ì‹œ) |
| ì´ìƒ íŒ¨ì¹˜ | Î± * ctx | gate â†’ 1 (context ì‚¬ìš©) |

### Code Changes

#### 1. MoLEContextSubnet (lora.py)

**Before (Baseline 1.5 - Global Alpha)**:
```python
class MoLEContextSubnet(nn.Module):
    def __init__(self, dims_in, dims_out, ...,
                 context_init_scale=0.1, context_max_alpha=0.2):
        # ...

        # Global alpha with sigmoid upper bound
        # alpha = alpha_max * sigmoid(alpha_param)
        p = min(max(context_init_scale / context_max_alpha, 0.01), 0.99)
        init_param = torch.log(torch.tensor([p / (1 - p)]))  # Inverse sigmoid
        self.context_scale_param = nn.Parameter(init_param)

    def forward(self, x):
        # ...
        ctx = self.context_conv(x_spatial)

        # Global alpha scaling (same for ALL patches)
        alpha = self.context_max_alpha * torch.sigmoid(self.context_scale_param)
        ctx = alpha * ctx  # (BHW, D) * scalar

        s_input = torch.cat([x, ctx], dim=-1)
        # ...
```

**After (Baseline 2.0 - Patch-wise Gate)**:
```python
class MoLEContextSubnet(nn.Module):
    def __init__(self, dims_in, dims_out, ...,
                 context_init_scale=0.1, context_max_alpha=0.2,
                 use_context_gate=False, context_gate_hidden=64):  # NEW
        # ...
        self.use_context_gate = use_context_gate

        if use_context_gate:
            # NEW: Patch-wise gate network
            # gate = sigmoid(MLP([x, ctx])) â†’ (BHW, 1)
            self.context_gate_net = nn.Sequential(
                nn.Linear(dims_in * 2, context_gate_hidden),
                nn.ReLU(),
                nn.Linear(context_gate_hidden, 1)
            )
            # Initialize to output ~0 â†’ gate starts at 0.5
            nn.init.zeros_(self.context_gate_net[0].weight)
            nn.init.zeros_(self.context_gate_net[0].bias)
            nn.init.zeros_(self.context_gate_net[2].weight)
            nn.init.zeros_(self.context_gate_net[2].bias)

            self.context_scale_param = None  # No global alpha
        else:
            # Legacy: Global alpha (Baseline 1.5)
            p = min(max(context_init_scale / context_max_alpha, 0.01), 0.99)
            init_param = torch.log(torch.tensor([p / (1 - p)]))
            self.context_scale_param = nn.Parameter(init_param)
            self.context_gate_net = None

    def forward(self, x):
        # ...
        ctx = self.context_conv(x_spatial)

        if self.use_context_gate and self.context_gate_net is not None:
            # NEW: Patch-wise gate (per-patch decision)
            gate_input = torch.cat([x, ctx], dim=-1)  # (BHW, 2D)
            gate_logit = self.context_gate_net(gate_input)  # (BHW, 1)
            gate = torch.sigmoid(gate_logit)  # (BHW, 1)

            self._last_gate = gate.detach()  # For logging
            ctx = gate * ctx  # (BHW, D) * (BHW, 1) â†’ per-patch scaling
        else:
            # Legacy: Global alpha
            alpha = self.context_max_alpha * torch.sigmoid(self.context_scale_param)
            ctx = alpha * ctx

        s_input = torch.cat([x, ctx], dim=-1)
        # ...

    # NEW: Logging utilities
    def get_context_alpha(self) -> float:
        """Get global alpha value (legacy mode)."""
        if self.context_scale_param is not None:
            with torch.no_grad():
                return (self.context_max_alpha
                        * torch.sigmoid(self.context_scale_param)).item()
        return None

    def get_last_gate_stats(self) -> dict:
        """Get gate statistics (patch-wise mode)."""
        if hasattr(self, '_last_gate') and self._last_gate is not None:
            gate = self._last_gate
            return {
                'mean': gate.mean().item(),
                'std': gate.std().item(),
                'min': gate.min().item(),
                'max': gate.max().item()
            }
        return None
```

#### 2. AblationConfig (ablation.py)

**Added**:
```python
@dataclass
class AblationConfig:
    # ... existing fields ...

    # Scale-specific Context (Baseline 1.5)
    use_scale_context: bool = False
    scale_context_kernel: int = 3
    scale_context_init_scale: float = 0.1
    scale_context_max_alpha: float = 0.2

    # NEW: Patch-wise Context Gate (Baseline 2.0)
    use_context_gate: bool = False    # Use patch-wise gate instead of global alpha
    context_gate_hidden: int = 64     # Hidden dim for gate MLP
```

#### 3. MoLESpatialAwareNF (mole_nf.py)

**Before**:
```python
subnet = MoLEContextSubnet(
    dims_in, dims_out,
    rank=self.lora_rank,
    alpha=self.lora_alpha,
    use_lora=self.use_lora,
    use_task_bias=self.use_task_bias,
    context_kernel=self.scale_context_kernel,
    context_init_scale=self.scale_context_init_scale,
    context_max_alpha=self.scale_context_max_alpha
)
```

**After**:
```python
subnet = MoLEContextSubnet(
    dims_in, dims_out,
    rank=self.lora_rank,
    alpha=self.lora_alpha,
    use_lora=self.use_lora,
    use_task_bias=self.use_task_bias,
    context_kernel=self.scale_context_kernel,
    context_init_scale=self.scale_context_init_scale,
    context_max_alpha=self.scale_context_max_alpha,
    use_context_gate=self.use_context_gate,      # NEW
    context_gate_hidden=self.context_gate_hidden  # NEW
)
```

**Added get_context_info() method**:
```python
def get_context_info(self) -> dict:
    """Get context gate/alpha information for logging."""
    if not self.use_scale_context:
        return {}

    info = {}
    if self.use_context_gate:
        # Aggregate gate stats from all subnets
        gate_stats = []
        for subnet in self.subnets:
            if hasattr(subnet, 'get_last_gate_stats'):
                stats = subnet.get_last_gate_stats()
                if stats is not None:
                    gate_stats.append(stats)

        if gate_stats:
            info['gate_mean'] = sum(s['mean'] for s in gate_stats) / len(gate_stats)
            info['gate_std'] = sum(s['std'] for s in gate_stats) / len(gate_stats)
            info['gate_min'] = min(s['min'] for s in gate_stats)
            info['gate_max'] = max(s['max'] for s in gate_stats)
    else:
        # Collect alpha from all subnets
        alphas = [s.get_context_alpha() for s in self.subnets
                  if hasattr(s, 'get_context_alpha')]
        alphas = [a for a in alphas if a is not None]
        if alphas:
            info['alpha_mean'] = sum(alphas) / len(alphas)

    return info
```

#### 4. Trainer Logging (continual_trainer.py)

**Added context logging per epoch**:
```python
# In _train_base_task, _train_fast_stage, _train_slow_stage:
avg_epoch_loss = epoch_loss / max(num_batches, 1)
current_lr = optimizer.param_groups[0]['lr']

# NEW: Get context gate/alpha info for logging
extra_info = {"LR": current_lr}
context_info = self.nf_model.get_context_info()
if context_info:
    extra_info.update(context_info)

if self.logger:
    self.logger.log_epoch(task_id, epoch, num_epochs, avg_epoch_loss,
                          stage="FAST", extra_info=extra_info)
else:
    ctx_str = ""
    if 'gate_mean' in context_info:
        ctx_str = f" | Gate: {context_info['gate_mean']:.4f}Â±{context_info['gate_std']:.4f}"
    elif 'alpha_mean' in context_info:
        ctx_str = f" | Alpha: {context_info['alpha_mean']:.4f}"
    print(f"  ðŸ“Š [FAST] Epoch [...] Average Loss: {avg_epoch_loss:.4f}{ctx_str}")
```

### Command Line Usage

```bash
# Baseline 1.5: Global Alpha (ê¸°ì¡´)
python run_moleflow.py \
    --use_scale_context \
    --scale_context_kernel 3 \
    --scale_context_init_scale 0.1 \
    --scale_context_max_alpha 0.2

# Baseline 2.0: 
